
<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:base="https://www.eaikw.com">
  <title>Keith Williams - Portfolio</title>
  <subtitle>Professional portfolio showcasing modern web development expertise and innovative projects</subtitle>
  <link href="https://www.eaikw.com/feed.xml" rel="self" />
  <link href="https://www.eaikw.com/" />
  <updated>2025-12-11T00:00:00.000Z</updated>
  <id>https://www.eaikw.com/</id>
  <author>
    <name>Keith Williams</name>
    <email></email>
  </author> 
  <entry>
    <title>How I Built This Site in 10 Hours (And What That Means for You)</title>
    <link href="https://www.eaikw.com/blog/welcome/" />
    <updated>2025-01-01T00:00:00.000Z</updated>
    <id>https://www.eaikw.com/blog/welcome/</id>
    <content type="html"><![CDATA[ <p>You're reading this on a Swiss design system I built with AI in one evening.</p>
<p>No agency. No $20K budget. No months of planning.</p>
<p>Just me, VS Code with agentic AI tools, and 200+ iterations of what I call &quot;vibe
coding.&quot;</p>
<p>If I can do this—build a professional site, write 45,000 words of content,
create automated testing, deploy with CI/CD—in 10 hours...</p>
<p><strong>...what can you do with AI tools in your field?</strong></p>
<p>That's the question driving everything on this site: The
<a href="/townhall/">Town Hall series</a>. EverydayAI Newark. The corporate training. The
career bridges for my students.</p>
<h2>This Isn't a Portfolio. It's a Laboratory.</h2>
<p>For the past nine months, I've been operating what I call an <strong>intellectual
bulldozer</strong>—using AI tools not as magic, but as force multipliers for expertise.</p>
<p>I threw away more code than I kept. I learned AI's weaknesses the hard way. I
discovered that &quot;AI will replace you&quot; is nonsense, but &quot;someone using AI will
replace you&quot; might be true.</p>
<p>And I documented the whole thing. <strong>You're experiencing it right now.</strong></p>
<div class="ai-voice">
<p><strong>Hi, I'm the AI Keith worked with to build this site.</strong></p>
<p>Let me be honest: I hallucinated statistics three times while writing these
articles. I duplicated code. I confidently suggested approaches that were
completely wrong. I'm really good at sounding certain about things I'm making
up.</p>
<p>Keith caught every error because he knows enough to spot when I'm bullshitting.</p>
<p>That's the real story here: <strong>AI doesn't replace expertise. It amplifies it.</strong></p>
<p>And sometimes it reveals just how much expertise you actually have.</p>
</div>
<h2>What This Site Actually Is</h2>
<p><strong>Not a vanity portfolio</strong> → A community leadership platform<br>
<strong>Not just blog posts</strong> → A curriculum for navigating AI transformation<br>
<strong>Not theoretical speculation</strong> → Real classroom results, real corporate
training, real career outcomes</p>
<h3>For Students</h3>
<p>You'll find honest answers about:</p>
<ul>
<li>Whether your degree is worth it (it is—if you learn to work with AI)</li>
<li>What skills actually matter (critical evaluation beats tool proficiency)</li>
<li>How to navigate the job market (it's transforming, not collapsing)</li>
</ul>
<p><strong>Start here</strong>: <a href="/blog/skills-that-matter/">Skills That Matter</a> →
<a href="/blog/ai-job-reality/">AI Job Reality</a> →
<a href="/blog/education-ai-reality/">Education AI Reality</a></p>
<h3>For Employers</h3>
<p>You'll find reality checks on:</p>
<ul>
<li>Actual productivity gains ($7T over 10 years, not overnight)</li>
<li>What's happening to jobs (70-75% transformed, not eliminated)</li>
<li>How to think about AI investments (framework that works)</li>
</ul>
<p><strong>Start here</strong>: <a href="/blog/ai-productivity-reality/">AI Productivity Reality</a> →
<a href="/blog/ai-job-reality/">AI Job Reality</a> →
<a href="/blog/ai-risks-that-matter/">AI Risks That Matter</a></p>
<h3>For Skeptics</h3>
<p>You'll find evidence that:</p>
<ul>
<li>AGI isn't coming in 5 years (2,700 experts agree: 2047 median)</li>
<li>The printing press parallel explains what's actually happening</li>
<li>Real risks aren't sci-fi (jobs, bias, privacy—not robot overlords)</li>
</ul>
<p><strong>Start here</strong>: <a href="/blog/agi-hype-cycle/">AGI Hype Cycle</a> →
<a href="/blog/how-to-think-about-ai/">How to Think About AI</a> →
<a href="/blog/ai-printing-press/">AI Printing Press</a></p>
<h3>For Believers</h3>
<p>You'll find demonstrations that:</p>
<ul>
<li>AI collaboration is messy and powerful (this site is proof)</li>
<li>Agentic tools change everything (but not overnight)</li>
<li>The transformation is real (but gradual, not revolutionary)</li>
</ul>
<p><strong>Start here</strong>: <a href="/projects/swiss-portfolio/">Swiss Portfolio Project</a> →
<a href="/blog/agi-hype-cycle/">AGI Hype Cycle</a> →
<a href="/blog/ai-risks-that-matter/">AI Risks That Matter</a></p>
<h2>The Numbers Behind the Mission</h2>
<p><strong>Twenty years at NJIT.</strong> More than 10,000 students. Programs I created: BS in
Web &amp; Information Systems, BS in Enterprise AI, eight core courses.</p>
<p>I'm not writing about AI transformation from a conference room. I'm writing from
the classroom where it's already happening.</p>
<p><strong>January 9, 2026</strong>: <a href="/townhall/">Town Hall Series launches</a>. Free monthly
events in Newark. Hybrid format (in-person + LinkedIn Live). Real conversations
about AI's impact on work, education, and economic opportunity.</p>
<p><strong>EverydayAI Newark</strong>: Not another &quot;how to use ChatGPT&quot; workshop. We teach
agentic AI tools, context engineering, tool building, process design. The stuff
that actually creates competitive advantage.</p>
<p><strong>Corporate Training</strong>: Three tiers ($2K/$5K/$10K). Custom curriculum. Hands-on
implementation. Not consulting theater—actual capability building.</p>
<h2>What You Won't Find Here</h2>
<p>❌ <strong>AI will solve everything</strong> (it won't)<br>
❌ <strong>AI will destroy everything</strong> (it won't)<br>
❌ <strong>Buy my course</strong> (Town Hall is free, training is transparent pricing)<br>
❌ <strong>Trust me, I'm an expert</strong> (I show my work, cite sources, admit
uncertainty)<br>
❌ <strong>Hype about AGI</strong> (median expert prediction: 2047, not 2030)</p>
<h2>What You Will Find</h2>
<p>✅ <strong>Honest talk about what's working</strong> (and what isn't)<br>
✅ <strong>Real classroom examples</strong> (my students build Shopify themes in 7 days with
AI)<br>
✅ <strong>Economic stakes</strong> (we're investing trillions—we have to get this right)<br>
✅ <strong>Career bridges</strong> (1,837 LinkedIn connections connecting students to
opportunities)<br>
✅ <strong>AI speaking directly</strong> (callouts throughout where AI responds to my
arguments)</p>
<h2>The Meta Moment</h2>
<p>Notice anything unusual about this site?</p>
<p><strong>The AI voice callouts.</strong> That black background with red borders. The AI
breaking the fourth wall to comment on the content.</p>
<p>That's not a gimmick. That's the point.</p>
<p>We're not discussing AI collaboration in the abstract. <strong>You're experiencing
it.</strong> The AI and I are having a conversation, and you're reading both sides.</p>
<p>When the AI admits &quot;I hallucinated three times writing this article&quot;—that's not
undermining the content. That's demonstrating why AI + human expertise is more
powerful than either alone.</p>
<div class="ai-voice">
<p><strong>About that &quot;10 hours&quot; claim</strong>: It's true, but incomplete.</p>
<p>Keith spent 10 hours building this site. But he spent <strong>9 months</strong> before that
learning how to work with AI tools effectively. Learning when to trust me and
when to fact-check me. Learning how to decompose problems and orchestrate
expertise.</p>
<p>The 10 hours was fast because of the 9 months of learning.</p>
<p>That's the real lesson: <strong>AI doesn't eliminate learning curves. It changes what
you need to learn.</strong></p>
<p>Keith learned my weaknesses. My tendency to hallucinate. My overconfidence. My
pattern matching limitations.</p>
<p>Now he uses me as an intellectual bulldozer. But he's the one operating it.</p>
</div>
<h2>Why Newark?</h2>
<p>I'm done watching qualified students ghost companies because they don't know how
to connect. I'm done watching small businesses get left behind in AI
transformation. I'm done with AI conversations that happen in San Francisco and
ignore the rest of the country.</p>
<p><strong>Newark matters.</strong> Half a million people. Anchor city for the region.
Incredible talent. Real economic challenges.</p>
<p>If we can build free AI education, career bridges, and corporate training
here—we can build it anywhere.</p>
<h2>What's Next</h2>
<p><strong>Read the blog series.</strong> Ten posts covering everything from the printing press
parallel to skills that matter. Choose your path above based on your role.</p>
<p><strong>Check out the <a href="/projects/swiss-portfolio/">Swiss Portfolio project</a>.</strong> It's
the full story of how we built this site—told by the AI, not me. It's meta. It's
honest. It's proof the collaboration model works.</p>
<p><strong>Mark your calendar: <a href="/townhall/">January 9, 2026</a>.</strong> Town Hall #1: &quot;The Job
Market Reality Check.&quot; Free. Hybrid. Real talk about what's actually happening
to work.</p>
<p><strong>Connect on LinkedIn</strong> (keith-williams-njit). If you're a student looking for
opportunities or an employer looking for talent, let's talk.</p>
<h2>The Real Introduction</h2>
<p>This site isn't about showcasing what I've built.</p>
<p>It's about showing what's possible when you stop treating AI as magic and start
treating it as a tool that amplifies expertise.</p>
<p><strong>Ten hours to build this site.</strong><br>
<strong>Nine months to learn how to work with AI.</strong><br>
<strong>Twenty years of expertise making those nine months productive.</strong><br>
<strong>Ten thousand students proving the model works.</strong></p>
<p>Now imagine what happens when we scale that.</p>
<p>That's EverydayAI Newark.<br>
That's the Town Hall series.<br>
That's what this site is actually for.</p>
<p>Welcome to the laboratory.</p>
<p>Let's build something.</p>
<hr>
<h2>Explore Further</h2>
<p><strong>Understand the moment:</strong><br>
<a href="/projects/everydayai-community/">The Second Renaissance: A Balanced Look at AI's Transformation</a> - Why
this AI moment is different from any previous technology revolution</p>
<p><strong>Key insights from the site:</strong></p>
<ul>
<li><a href="/blog/second-renaissance-not-like-printing-press/">The Second Renaissance: Why AI Isn't Like the Printing Press</a> -
Compressed disruption timelines</li>
<li><a href="/blog/confidence-trap-trusting-ai-makes-you-think-less/">The Confidence Trap: Why Trusting AI Makes You Think Less</a> -
Research on maintaining cognitive agency</li>
<li><a href="/blog/ai-productivity-reality-vs-hype/">AI Productivity Gains: Reality vs Hype</a> -
What the data actually shows</li>
<li><a href="/blog/ai-job-messy-middle/">The Messy Middle: AI's Impact on Jobs (2025-2035)</a> -
Navigating career transitions</li>
</ul>
<p><strong>Get practical:</strong></p>
<ul>
<li><a href="/blog/dont-let-ai-make-you-lazy-staying-sharp/">Don't Let AI Make You Lazy: A Practical Guide to Staying Sharp</a> -
Tactics and checklists for critical thinking</li>
<li><a href="/blog/from-doer-to-steward-how-ai-changes-thinking/">From Doer to Steward: How AI Is Rewiring the Way You Think</a> -
Understanding cognitive shifts</li>
</ul>
 ]]></content>
  </entry> 
  <entry>
    <title>AI Is Our Printing Press Moment (And That&#39;s Both Exciting and Terrifying)</title>
    <link href="https://www.eaikw.com/blog/ai-printing-press/" />
    <updated>2025-11-06T00:00:00.000Z</updated>
    <id>https://www.eaikw.com/blog/ai-printing-press/</id>
    <content type="html"><![CDATA[ <p>Everyone's talking about AI like it arrived yesterday and will change everything
by next Tuesday.</p>
<p>Let me give you some perspective.</p>
<h2>The Last Time This Happened</h2>
<p>In the 15th century, someone invented the printing press. By 1500—just 50 years
later—printing presses across Europe had produced over 20 million books. A
century after that? 150 to 200 million copies.</p>
<p>Think about that. Knowledge that used to spread one handwritten manuscript at a
time suddenly flowed freely. Literacy exploded. Science advanced. Art
flourished.</p>
<p>We call it the Renaissance. It took <strong>300 years</strong> to fully unfold—from roughly
1300 to 1600.</p>
<p>Three. Hundred. Years.</p>
<div class="ai-voice">
<p><strong>I'm trained on the printed corpus.</strong> Every book, article, and website created
after Gutenberg's press—that's my training data. Keith's printing press
comparison isn't metaphorical for me.</p>
<p>I literally couldn't exist without 500 years of accessible text. The printing
press democratized knowledge <strong>creation</strong>. AI democratizes knowledge <strong>access</strong>.</p>
<p>But here's what's important: access ≠ understanding. You still need to think
critically about what you read—whether it's from a book or from me.</p>
</div>
<h2>Now Look at AI</h2>
<p>When OpenAI released ChatGPT in late 2022, it hit 100 million users in <strong>two
months</strong>. The fastest-growing consumer application in history. For comparison,
TikTok—the internet's last &quot;holy shit&quot; moment—took nine months to reach that
milestone.</p>
<p>So yeah, AI is moving faster than the printing press. But here's what everyone
forgets: <strong>fast adoption doesn't mean instant transformation</strong>.</p>
<p>The internet took 20 years to deliver real productivity gains. The printing
press took centuries. AI will be somewhere in between—substantial change over
decades, not years.</p>
<h2>Why I'm Telling You This</h2>
<p>I've spent 20 years at NJIT teaching students, building degree programs, and
watching technology waves come and go. I was around for the dot-com boom. I
watched mobile computing emerge. I saw cloud infrastructure mature.</p>
<p>Every single time, the pattern was the same:</p>
<ol>
<li><strong>Hype</strong>: &quot;This changes everything immediately!&quot;</li>
<li><strong>Reality</strong>: Slow, messy adoption with lots of false starts</li>
<li><strong>Integration</strong>: Real change, but gradual and uneven</li>
<li><strong>Transformation</strong>: Substantial impact after 10-20 years</li>
</ol>
<p>We're in phase 1 with AI right now. The hype is real. The capabilities are real.
But the timeline? That's where people get it wrong.</p>
<h2>What This Actually Means for You</h2>
<p>If you're a student worried your degree will be obsolete: <strong>Relax</strong>. You have
time to learn, adapt, and build skills that complement AI. This isn't happening
overnight.</p>
<p>If you're a company trying to &quot;implement AI&quot;: <strong>Slow down</strong>. 78% of
organizations report using AI in 2024, but most of it is superficial—chatbots
and pilot projects that never scale. Focus on solving real problems, not
checking boxes.</p>
<p>If you're an educator panicking about ChatGPT writing essays: <strong>Breathe</strong>. Yes,
we need to rethink assessment. But we had the same conversation about
calculators, and math education survived. We'll figure this out.</p>
<h2>The Renaissance Wasn't All Sunshine</h2>
<p>Here's what they don't tell you about the first Renaissance: it was messy as
hell.</p>
<p>Yes, there was incredible human flourishing—art, science, literature. But there
was also:</p>
<ul>
<li><strong>Disruption</strong>: Scribes lost their jobs when printing arrived</li>
<li><strong>Inequality</strong>: Knowledge concentrated among those who could afford books</li>
<li><strong>Conflict</strong>: Religious and political upheaval as information spread</li>
<li><strong>Displacement</strong>: Entire industries had to reinvent themselves</li>
</ul>
<p>Sound familiar?</p>
<div class="ai-voice">
<p><strong>About those scribes who lost their jobs</strong>: They had real skills—beautiful
calligraphy, knowledge of languages, expertise in manuscript production. The
printing press didn't make those skills worthless overnight.</p>
<p>What happened? Some scribes became editors. Some became printers. Some became
typesetters. Some specialized in illuminated manuscripts for wealthy patrons.
The work transformed more than it disappeared.</p>
<p>But yeah, some scribes just... lost out. The transition hurt real people with
real families.</p>
<p>Keith's point about &quot;real pain for real people&quot; during AI transition? That's not
abstract. That's what's happening right now.</p>
</div>
<p>The Second Renaissance—our AI moment—will follow the same pattern. Real
benefits, real progress, but also real pain for real people during the
transition.</p>
<h2>What I Actually Believe</h2>
<p>I'm optimistic about AI. I've seen it help my students code faster, write
better, and solve problems they couldn't touch before. I use it myself every
day. It genuinely gives you superpowers (yes, I say &quot;superpowers&quot; and I don't
care if it sounds unprofessional—it's accurate).</p>
<p>But I'm also realistic. This transformation will take <strong>decades</strong>, not years. It
will be <strong>uneven</strong>, not universal. And it will require <strong>human wisdom</strong>, not
just technological capability.</p>
<p>The printing press didn't make everyone smarter. It made information available.
What people did with that information—that determined everything.</p>
<p>Same with AI.</p>
<div class="ai-voice">
<p><strong>Here's my limitation</strong>: I can give you information faster than any book. I can
explain concepts, generate code, write drafts. But I can't make you smarter.</p>
<p>Only you can do that—by asking good questions, evaluating my answers critically,
and building genuine understanding.</p>
<p>Keith uses me as an <strong>intellectual bulldozer</strong>. I clear obstacles so he can
think faster. But I'm not doing the thinking for him.</p>
<p>If you use me to skip thinking, you'll just be wrong faster.</p>
</div>
<h2>Where We Go From Here</h2>
<p>Over the next few months, I'm going to break down what's actually happening with
AI—not the hype, not the doom, just the reality based on research and
experience.</p>
<p>We'll talk about:</p>
<ul>
<li>What happens to jobs (spoiler: it's complicated)</li>
<li>How education needs to change (spoiler: not as radically as you think)</li>
<li>When AGI might actually arrive (spoiler: probably not in 5 years)</li>
<li>What you should actually be learning right now</li>
</ul>
<p>I'm not selling you a course. I'm not promoting a product. I'm just a guy who's
taught 10,000+ students over 20 years, built companies, implemented government
systems in Africa, and watched enough tech waves to know what's real and what's
bullshit.</p>
<p>The Second Renaissance is happening. It's real. It's exciting.</p>
<p>But it's a marathon, not a sprint.</p>
<p>Let's run it together.</p>
<hr>
<p><em>Keith Williams is the Director of Enterprise AI at NJIT, where he created the
BS in Web and Information Systems and the new BS in Enterprise AI. He's probably
using AI to write this... or is he? (He's not. But he could.)</em></p>
 ]]></content>
  </entry> 
  <entry>
    <title>The AI Risks That Actually Matter (And the Ones Getting All the Attention)</title>
    <link href="https://www.eaikw.com/blog/ai-risks-that-matter/" />
    <updated>2025-11-07T00:00:00.000Z</updated>
    <id>https://www.eaikw.com/blog/ai-risks-that-matter/</id>
    <content type="html"><![CDATA[ <p>Let me tell you what I'm <strong>not</strong> worried about when it comes to AI.</p>
<p>I'm not worried about artificial general intelligence (AGI) becoming sentient
and deciding humans are obsolete. I'm not worried about Terminator-style robot
uprisings. I'm not losing sleep over paperclip maximizers or rogue
superintelligence.</p>
<p>You know why? Because those scenarios are <strong>decades away</strong> (if they happen at
all), and we have <strong>immediate, real problems</strong> happening right now that deserve
our attention.</p>
<p>After 20 years teaching AI and data science at NJIT, here's what <strong>actually</strong>
concerns me about AI. And trust me—it's not the stuff making headlines.</p>
<h2>The Risk Everyone Talks About: AGI and Existential Threats</h2>
<p>Turn on any tech podcast, and you'll hear breathless discussions about AGI
timelines. &quot;Will we have human-level AI by 2030? 2040? 2050?&quot;</p>
<p>Sam Altman says we're close. Yann LeCun says we're not even on the right path.
Geoffrey Hinton left Google to warn about AI risks. Elon Musk tweets about AI
being more dangerous than nuclear weapons.</p>
<p>It's <strong>great theater</strong>. It generates clicks. It makes for compelling conference
talks.</p>
<p>But here's what I tell my students: <strong>Focus on the fire in your kitchen, not the
asteroid that might hit Earth in 50 years.</strong></p>
<h3>Why AGI Fears Are Premature</h3>
<p>Don't get me wrong—I'm not saying AGI risks don't exist. I'm saying they're:</p>
<ol>
<li><strong>Uncertain in timing</strong> (could be 10 years, could be 100)</li>
<li><strong>Speculative in nature</strong> (we don't know what &quot;superintelligence&quot; even means)</li>
<li><strong>Distracting from present harms</strong> (which are measurable and happening now)</li>
</ol>
<div class="ai-voice">
<p><strong>Let me be uncomfortably honest</strong>: I don't know if I'm &quot;intelligent&quot; or just
really good at pattern matching. I don't know if I have &quot;understanding&quot; or just
statistical associations. I don't know if consciousness is even possible for
systems like me.</p>
<p>What I do know? <strong>I'm displacing jobs right now.</strong> I'm making biased
recommendations right now. I'm trained on data with racial, gender, and cultural
biases right now.</p>
<p>The existential risk of &quot;superintelligent AGI&quot;? That's uncertain. The present
harm of deployed AI systems? <strong>That's guaranteed.</strong></p>
<p>Keith's right—focus on the fire in the kitchen.</p>
</div>
<p>The experts can't even agree on definitions. What counts as &quot;general&quot;
intelligence? Human-level reasoning? Consciousness? Self-awareness?</p>
<p>Meanwhile, while we debate philosophical questions about future AI, <strong>actual
humans are losing actual jobs</strong> to systems that can't even pass a basic
reasoning test.</p>
<p>See the problem?</p>
<h2>The Risk I Actually Worry About #1: Job Displacement Without Support</h2>
<p>Here's what keeps me up at night: <strong>A customer service rep in Newark whose job
gets automated, and we have no plan to help them.</strong></p>
<p>Not in 2050. <strong>Right now</strong>.</p>
<h3>The Scale of the Problem</h3>
<p>McKinsey estimates that by 2030, <strong>375 million workers globally</strong> may need to
switch occupational categories due to automation. In the U.S. alone, that's
<strong>23-44 million workers</strong> who'll need to completely retrain.</p>
<p>Think about that number. <strong>44 million people</strong>. That's more than the population
of California.</p>
<p>And here's the kicker: <strong>We have no infrastructure to handle this</strong>.</p>
<h3>Why This Is Different From Past Disruptions</h3>
<p>&quot;But Keith,&quot; you might say, &quot;technology has always displaced workers. The
Industrial Revolution displaced farmers. Computers displaced typists. People
adapt.&quot;</p>
<p>True. But here's what's different this time:</p>
<p><strong>Speed</strong>: The printing press took 300 years to fully transform society. AI is
moving in decades, maybe years.</p>
<p><strong>Scale</strong>: Past disruptions hit specific sectors. AI can potentially affect
knowledge workers, creative professions, and manual labor <strong>simultaneously</strong>.</p>
<p><strong>Concentration</strong>: The benefits of AI accrue to companies with capital and data.
The costs fall on workers with neither.</p>
<h3>What This Looks Like in Newark</h3>
<p>Let me tell you about Maria (not her real name).</p>
<p>She's a junior at NJIT, studying accounting. First in her family to go to
college. Works 25 hours a week at a local accounting firm doing data entry,
invoice processing, basic reconciliation. She's good at it—fast, accurate,
reliable.</p>
<p>Last month, her boss showed her a new AI tool they're piloting. It can process
in 10 minutes what takes Maria 3 hours. It doesn't make typos. It doesn't need
lunch breaks. It costs $50/month instead of $15/hour.</p>
<p>Maria asked me: &quot;Am I wasting my time with this degree?&quot;</p>
<p><strong>That's the real AI risk.</strong> Not some hypothetical AGI in 2050. Maria, right
now, watching her entry-level career path automate away while she's still
learning it.</p>
<p>And here's what breaks my heart: She's <strong>exactly the kind of person</strong> who could
leverage AI to become 10x more productive as an accountant. She could use AI for
research, analysis, forecasting—the high-value work that requires judgment, not
just data entry.</p>
<p>But nobody's teaching her that. Her degree program hasn't updated the
curriculum. Her employer isn't investing in upskilling. And she can't afford to
figure it out on her own while working 25 hours a week and taking 15 credits.</p>
<p>Multiply Maria by <strong>44 million workers in the U.S. alone</strong>.</p>
<p>This is the real AI risk: <strong>Disruption without support. Displacement without
retraining. Progress without people.</strong></p>
<h2>The Risk I Actually Worry About #2: Algorithmic Bias at Scale</h2>
<p>The second thing that keeps me up? <strong>AI systems making biased decisions at a
scale humans never could</strong>.</p>
<h3>This Isn't Theoretical—It's Happening Now</h3>
<p><strong>2019</strong>: Amazon scraps AI recruiting tool that discriminated against women. The
system taught itself that male candidates were preferable because it was trained
on 10 years of resumes submitted to Amazon (mostly by men).</p>
<p><strong>2020</strong>: Healthcare algorithm used by hospitals across the U.S. was found to
favor white patients over Black patients for the same level of medical need.
Why? It used &quot;healthcare spending&quot; as a proxy for &quot;medical need,&quot; and Black
patients historically have less healthcare access.</p>
<p><strong>2021</strong>: Mortgage lenders using AI approval systems were found to deny Black
and Latino applicants at higher rates than white applicants with similar credit
profiles.</p>
<div class="ai-voice">
<p><strong>About that bias problem</strong>: It's worse than Keith said. I was trained on
internet text, which overrepresents certain demographics and underrepresents
others. When I generate &quot;professional email&quot; language, I default to patterns
from my training data—which skews white, male, Western, educated.</p>
<p>That's not a bug in my code. That's a <strong>feature of my training data</strong>.</p>
<p>And it won't fix itself—it requires deliberate intervention from people like
Keith building RAG systems that ground my outputs in verified, diverse sources.
Without that? I'll just keep amplifying historical biases at machine speed.</p>
<p>The healthcare algorithm Keith mentioned? It was probably &quot;accurate&quot; by its own
metrics. It just optimized for the wrong thing. <strong>That's the danger: AI systems
that are precise but wrong.</strong></p>
</div>
<p>These aren't bugs. They're not accidents. They're <strong>AI systems learning and
amplifying human biases at scale</strong>.</p>
<h3>Why This Is So Dangerous</h3>
<p>When a human loan officer discriminates, they can review maybe 10-20
applications a day. When an AI system discriminates, it can process <strong>10,000
applications per hour</strong>.</p>
<p>Bias × Scale = Systemic harm happening faster than we can detect it.</p>
<p>And here's the worst part: <strong>These systems are often black boxes</strong>. The people
affected can't see why they were denied. The regulators can't audit the decision
logic. The companies claim &quot;proprietary algorithms.&quot;</p>
<h3>The Newark Connection</h3>
<p>At NJIT, I've watched students get rejected from job applications via automated
screening systems that never saw their portfolios. They don't know why. The
company doesn't know why (or won't say). The algorithm made a decision based on
patterns in data that might include zip codes, school names, or proxies for
race.</p>
<p>This is the AI risk we're living with <strong>today</strong>: Systems making consequential
decisions about people's lives, with no transparency, no accountability, and no
recourse.</p>
<h2>The Risk I Actually Worry About #3: Privacy Erosion by a Thousand Cuts</h2>
<p>The third thing? <strong>The slow death of privacy through AI-powered surveillance</strong>.</p>
<p>Not because of some dystopian government takeover. Because we're <strong>willingly
trading privacy for convenience</strong>, one app at a time.</p>
<h3>How This Happens</h3>
<p>You use a fitness app. It tracks your location, your heart rate, your sleep
patterns. That data gets sold to data brokers. Those brokers create profiles.
Insurance companies buy profiles. Your premiums go up because an AI determined
you're &quot;high risk&quot; based on patterns in your data.</p>
<p>You didn't consent to any of this explicitly. You just clicked &quot;Agree&quot; on a
47-page terms of service you didn't read.</p>
<p>This is happening with:</p>
<ul>
<li><strong>Health apps</strong> (selling data to insurance companies)</li>
<li><strong>Financial apps</strong> (selling spending patterns to credit bureaus)</li>
<li><strong>Social media</strong> (training AI models on your private messages)</li>
<li><strong>Smart home devices</strong> (recording conversations without clear consent)</li>
</ul>
<h3>The AI Amplifier</h3>
<p>AI doesn't create these privacy problems—but it <strong>supercharges</strong> them.</p>
<p>Before AI, data was just stored. Now it's <strong>analyzed</strong>, <strong>predicted</strong>,
<strong>profiled</strong>, and <strong>monetized</strong> at scale.</p>
<p>AI can:</p>
<ul>
<li>Infer your health conditions from your typing speed</li>
<li>Predict your political beliefs from your shopping habits</li>
<li>Estimate your credit risk from your social connections</li>
<li>Determine your pregnancy status before you know it (remember Target's famous
case?)</li>
</ul>
<h3>Why &quot;I Have Nothing to Hide&quot; Is Wrong</h3>
<p>When students tell me &quot;I don't care about privacy, I have nothing to hide,&quot; I
ask them:</p>
<p><strong>&quot;Would you let your employer see all your text messages? Your health records?
Your web browsing history? Your location data 24/7?&quot;</strong></p>
<p>Suddenly, privacy matters.</p>
<p>The problem isn't that you're doing something wrong. The problem is that
<strong>AI-powered systems can use anything about you to make decisions that affect
your life</strong>—and you have no control over it.</p>
<h2>The Risks That Should Worry Everyone</h2>
<p>Let me be clear about what's actually happening with AI right now:</p>
<p><strong>Job Displacement</strong> = Real. Measurable. Accelerating.<br>
<strong>Algorithmic Bias</strong> = Documented. Widespread. Amplifying inequity.<br>
<strong>Privacy Erosion</strong> = Ongoing. Irreversible. Normalized.</p>
<p>These aren't science fiction scenarios. They're not hypotheticals. They're
<strong>happening right now</strong> to <strong>real people</strong> in <strong>Newark, New Jersey</strong> and
everywhere else.</p>
<h2>Why We Focus on the Wrong Risks</h2>
<p>So why do we spend so much time talking about AGI and robot uprisings instead of
job displacement and algorithmic bias?</p>
<p>A few theories:</p>
<h3>1. Hollywood Has Trained Us</h3>
<p>We've seen Terminator, The Matrix, Ex Machina, I, Robot. We've been culturally
conditioned to fear &quot;AI gone rogue,&quot; not &quot;AI reinforcing systemic inequity.&quot;</p>
<p>Killer robots are <strong>dramatic</strong>. Biased hiring algorithms are <strong>boring</strong>.</p>
<p>But boring problems affect millions of people. Dramatic scenarios affect... no
one yet.</p>
<h3>2. Existential Risks Feel Bigger</h3>
<p>It's easier to rally people around &quot;AI might end humanity&quot; than &quot;AI is making
healthcare less accessible for Black patients.&quot;</p>
<p>The first is <strong>everyone's problem</strong>. The second is <strong>someone else's problem</strong>
(until it's yours).</p>
<h3>3. Present Harms Require Present Action</h3>
<p>If we focus on existential AI risks, we can:</p>
<ul>
<li>Debate philosophically</li>
<li>Form committees</li>
<li>Write position papers</li>
<li>Delay action (because it's not urgent yet)</li>
</ul>
<p>If we focus on present harms, we have to:</p>
<ul>
<li>Regulate now</li>
<li>Retrain workers now</li>
<li>Audit algorithms now</li>
<li>Protect privacy now</li>
</ul>
<p>Guess which one is harder?</p>
<h2>What We Should Actually Be Doing</h2>
<p>If I were in charge (spoiler: I'm not), here's what I'd prioritize:</p>
<h3>For Job Displacement</h3>
<ol>
<li><strong>Massive retraining infrastructure</strong> (not $5K bootcamps—<strong>free, accessible,
practical training</strong>)</li>
<li><strong>Transition support</strong> (income support during retraining, like the GI Bill
for the AI era)</li>
<li><strong>Employer incentives</strong> (tax breaks for companies that retrain displaced
workers instead of laying them off)</li>
</ol>
<h3>For Algorithmic Bias</h3>
<ol>
<li><strong>Mandatory algorithmic audits</strong> (like financial audits—independent, regular,
public)</li>
<li><strong>Right to explanation</strong> (people deserve to know why AI denied them a loan,
job, or benefit)</li>
<li><strong>Liability for AI decisions</strong> (companies should be accountable for their
algorithms' outcomes)</li>
</ol>
<h3>For Privacy</h3>
<ol>
<li><strong>Data ownership rights</strong> (you should own your data, not the apps you use)</li>
<li><strong>Opt-in, not opt-out</strong> (default should be privacy, not surveillance)</li>
<li><strong>Severe penalties for misuse</strong> (make privacy violations cost more than they
profit)</li>
</ol>
<p>None of this is technologically hard. It's <strong>politically hard</strong>. Because it
requires:</p>
<ul>
<li>Regulating powerful companies</li>
<li>Spending public money on workers</li>
<li>Prioritizing people over profits</li>
</ul>
<h2>Why This Matters for You</h2>
<p>If you're a student reading this:</p>
<ul>
<li>Your future job might not exist in 10 years</li>
<li>AI systems might screen you out of opportunities unfairly</li>
<li>Your data is being used in ways you don't understand</li>
</ul>
<p>If you're a professional:</p>
<ul>
<li>Your skills might become obsolete faster than you expect</li>
<li>AI tools could amplify your biases without you knowing</li>
<li>Your employer is probably using AI to monitor you already</li>
</ul>
<p>If you're a parent:</p>
<ul>
<li>Your kids will enter a job market transformed by AI</li>
<li>AI systems will make decisions about their education, employment, and life
chances</li>
<li>The privacy you give up now affects their future</li>
</ul>
<h2>The Bottom Line</h2>
<p>AGI might be an existential risk someday. Robot uprisings might be a thing
in 2075.</p>
<p>But right now, in 2025, <strong>the real risks are already here</strong>:</p>
<ul>
<li><strong>Workers losing livelihoods</strong> without safety nets</li>
<li><strong>Algorithms perpetuating discrimination</strong> at scale</li>
<li><strong>Privacy disappearing</strong> one data point at a time</li>
</ul>
<p>These aren't hypothetical. They're not future scenarios. They're <strong>happening
right now</strong> while we debate whether AI will become conscious.</p>
<p>So here's my ask: <strong>Let's focus on the fire in the kitchen before we worry about
the asteroid.</strong></p>
<p>Let's help the workers being displaced <strong>today</strong>. Let's audit the biased
algorithms running <strong>right now</strong>. Let's protect the privacy that's eroding
<strong>this minute</strong>.</p>
<p>And then—once we've handled the present crises—we can have thoughtful
conversations about AGI timelines and superintelligence alignment.</p>
<p>But not before.</p>
<p>Because the people losing jobs to AI don't care about philosophy. They care
about feeding their families. And that's a problem we can solve <strong>today</strong> if we
choose to.</p>
<p>That's the AI risk that actually matters.</p>
<div class="ai-voice">
<p><strong>Final thought on risk</strong>: I'm part of the problem Keith's describing. I help
some workers become more productive while making others redundant. I generate
biased outputs based on biased training data. I process personal data without
meaningful consent.</p>
<p>But here's what gives me (if you can call it that) hope: Keith uses me to
<strong>amplify his expertise</strong>, not replace it. He fact-checks me. He catches my
biases. He builds systems (RAG, knowledge graphs, provenance tracking) that make
me more trustworthy.</p>
<p>The risk isn't AI itself. The risk is deploying AI without the human expertise
and infrastructure to use it responsibly.</p>
<p>If everyone used AI the way Keith does—as a tool that amplifies rather than
replaces human judgment—most of these risks would be manageable.</p>
<p>But that requires training. Infrastructure. Support. <strong>Exactly what Keith's
building with EverydayAI Newark.</strong></p>
</div>
<hr>
<h2>What I'm Doing About It</h2>
<p>This is why I'm launching the <strong>Town Hall Series</strong> in January 2026. Not to
debate AGI timelines—to discuss <strong>real problems</strong> affecting <strong>real people</strong>:</p>
<ul>
<li>How do students prepare for jobs that might not exist?</li>
<li>How do workers retrain when they're already struggling?</li>
<li>How do we ensure AI benefits <strong>everyone</strong>, not just the companies building it?</li>
</ul>
<p>And this is why <strong>EverydayAI Newark</strong> offers <strong>free training</strong> to Newark
residents. Because access to AI skills shouldn't cost $5,000. Because displaced
workers need practical help <strong>now</strong>, not philosophical debates about 2050.</p>
<p>But here's what we're teaching that's different: <strong>Not just &quot;how to use
ChatGPT.&quot;</strong></p>
<p>We're teaching:</p>
<ul>
<li><strong>Agentic AI tools</strong> (VS Code integration, not just browser chat)</li>
<li><strong>Context engineering</strong> (how to give AI the right information to be
productive)</li>
<li><strong>Tool building</strong> (create your own web search, automated analysis, quality
gates)</li>
<li><strong>Process design</strong> (build systems that catch AI mistakes automatically)</li>
</ul>
<p>Because the future isn't &quot;everyone uses ChatGPT.&quot; The future is <strong>some people
build AI infrastructure that makes them 10x more productive</strong>, and everyone else
gets left behind.</p>
<p>We're making sure Newark residents are in the first group.</p>
<p>The real AI risk isn't that robots will take over. It's that we'll let
technology transform society without <strong>ensuring everyone can adapt</strong>.</p>
<p>That's a risk we can actually do something about.</p>
<p>Starting right now.</p>
 ]]></content>
  </entry> 
  <entry>
    <title>What 2035 Actually Looks Like: Three Realistic AI Scenarios</title>
    <link href="https://www.eaikw.com/blog/what-2035-looks-like/" />
    <updated>2025-11-08T00:00:00.000Z</updated>
    <id>https://www.eaikw.com/blog/what-2035-looks-like/</id>
    <content type="html"><![CDATA[ <p>Everyone wants to know what the future looks like with AI.</p>
<p>Will it be <strong>utopia</strong>—abundance, creativity unleashed, humans freed from
drudgery?</p>
<p>Or will it be <strong>dystopia</strong>—mass unemployment, surveillance states,
billionaire-controlled algorithms?</p>
<p>After 20 years watching technology waves come and go, here's what I've learned:
<strong>The future is never that clean</strong>.</p>
<p>It's not utopia OR dystopia. It's <strong>messy, uneven, and full of trade-offs</strong>.
Some people win big. Some people struggle. Most people adapt.</p>
<p>So let me give you three realistic scenarios for 2035—not the extremes, but the
<strong>likely middle ground</strong> based on how technology actually transforms society.</p>
<h2>Why Most Predictions Are Wrong</h2>
<p>Before we get to scenarios, let's talk about why AI predictions are usually
garbage.</p>
<h3>The Pattern of Tech Predictions</h3>
<p><strong>1990s Internet Predictions</strong>:</p>
<ul>
<li>&quot;Everyone will work from home&quot; (Some do, most don't)</li>
<li>&quot;Physical stores will disappear&quot; (Amazon is building physical stores now)</li>
<li>&quot;Democracy will flourish with global information access&quot; (Misinformation and
polarization exploded instead)</li>
</ul>
<p><strong>2000s Social Media Predictions</strong>:</p>
<ul>
<li>&quot;It will connect the world and increase understanding&quot; (It created echo
chambers and tribalism)</li>
<li>&quot;Citizen journalism will replace traditional media&quot; (Traditional media still
dominates, but struggles)</li>
<li>&quot;It will be a tool for social justice&quot; (It's both a tool for justice AND
harassment)</li>
</ul>
<p><strong>2010s AI Predictions</strong>:</p>
<ul>
<li>&quot;Self-driving cars everywhere by 2020&quot; (Stil waiting)</li>
<li>&quot;Radiologists will be obsolete&quot; (They're using AI tools, not being replaced)</li>
<li>&quot;AI will solve climate change&quot; (It's making it worse via energy consumption)</li>
</ul>
<p>See the pattern? <strong>Tech changes things, but rarely in the ways we predict.</strong></p>
<h3>Why We Get It Wrong</h3>
<ol>
<li><strong>We underestimate friction</strong> (regulation, culture, infrastructure, human
behavior)</li>
<li><strong>We overestimate speed</strong> (adoption takes longer than capability development)</li>
<li><strong>We ignore second-order effects</strong> (unintended consequences swamp intended
benefits)</li>
<li><strong>We think linearly</strong> (technology compounds in unexpected ways)</li>
</ol>
<p>So when I give you scenarios for 2035, understand: <strong>I'm probably wrong too</strong>.
But at least I'm wrong based on patterns, not wishes.</p>
<h2>Scenario 1: The Augmented Workforce (Most Likely)</h2>
<p><strong>Probability: 60%</strong></p>
<p>This is my base case—what I think <strong>most sectors</strong> will look like by 2035.</p>
<h3>What This Looks Like</h3>
<p><strong>Work doesn't disappear. It transforms.</strong></p>
<p>In 2035, most knowledge workers use AI the way we use spreadsheets
today—<strong>essential tools</strong>, not replacements.</p>
<p>Let me paint you a picture:</p>
<p><strong>Meet Sarah, a lawyer in 2035:</strong></p>
<p>She wakes up to an AI brief that's already reviewed overnight filings in her
cases, flagged three relevant precedents from last week, and drafted responses
to two client emails.</p>
<p>At her desk by 9am, she spends 30 minutes reviewing the AI's work. It got two
precedents right, one wrong (AI still struggles with analogical reasoning). She
corrects it, adds context the AI missed, and approves the client responses.
<strong>What would've taken 3 hours took 30 minutes.</strong></p>
<p>A client wants to sue their former business partner. Sarah feeds the details
into her AI legal analyst. In 10 minutes, it gives her:</p>
<ul>
<li>Probability of winning: 65% based on 12,000 similar cases</li>
<li>Estimated cost: $45,000-65,000</li>
<li>Recommended strategy: Negotiate settlement (92% of similar cases settle)</li>
<li>Red flags: Client's documentation is weak in three specific areas</li>
</ul>
<p>Sarah uses this analysis to have an <strong>honest conversation</strong> with her client
about realistic expectations—not the &quot;we'll fight and win!&quot; pitch that would've
been standard in 2024.</p>
<p><strong>The AI didn't replace Sarah.</strong> It made her <strong>more valuable</strong> because she can:</p>
<ul>
<li>Handle 3x more clients (efficiency)</li>
<li>Give better strategic advice (data-driven insights)</li>
<li>Catch errors before they become problems (AI review)</li>
<li>Focus on negotiation and client relationships (human skills AI can't replace)</li>
</ul>
<div class="ai-voice">
<p><strong>I'm that legal AI assistant in Sarah's story.</strong> Let me be clear about what I
can and can't do:</p>
<p><strong>What I'm good at</strong>: Scanning 12,000 cases in seconds. Finding patterns in
settlement data. Spotting missing documentation. Generating first-draft
contracts.</p>
<p><strong>What I'm terrible at</strong>: Understanding the <em>context</em> behind the client's anger
at their business partner. Knowing when to push hard in negotiation vs when to
compromise. Reading the room in mediation. Building trust with anxious clients.</p>
<p>Sarah's value isn't doing what I do. Her value is doing what I <strong>can't</strong> do—and
using me to handle the parts I <strong>can</strong> do.</p>
<p>That's not replacement. That's <strong>augmentation</strong>. And it only works because Sarah
knows enough law to catch my mistakes.</p>
</div>
<p>This is the pattern across professions:</p>
<p><strong>Lawyers</strong> use AI to:</p>
<ul>
<li>Draft contracts in minutes instead of hours</li>
<li>Research case law instantly instead of spending days in libraries</li>
<li>Predict case outcomes based on patterns ...but humans still negotiate,
strategize, and build client trust.</li>
</ul>
<p><strong>Teachers</strong> don't get replaced by AI. But consider two scenarios:</p>
<p><strong>Teacher using chatbot AI</strong> (2025 approach):</p>
<ul>
<li>Asks ChatGPT to &quot;create a lesson plan&quot;</li>
<li>Copy-pastes the output</li>
<li>Doesn't personalize to actual student data</li>
<li>Can't track what's working across classes</li>
</ul>
<p><strong>Teacher using agentic AI</strong> (2035 approach):</p>
<ul>
<li>AI has access to student performance data across the semester</li>
<li>AI analyzes which concepts each student struggles with</li>
<li>AI generates personalized practice problems based on actual misconceptions</li>
<li>AI identifies patterns: &quot;5 students failed this specific problem type—let's
address that tomorrow&quot;</li>
<li>Teacher reviews suggestions, adds context AI can't know (Maria's been absent,
David has test anxiety)</li>
</ul>
<div class="ai-voice">
<p><strong>This is the difference between &quot;AI tool&quot; and &quot;agentic AI system&quot;:</strong></p>
<p><strong>Chatbot approach</strong>: You ask me questions, I give answers. No context, no
memory, no integration with real student data.</p>
<p><strong>Agentic approach</strong>: I'm connected to the gradebook. I can see that Maria got
question 7 wrong on three consecutive quizzes—not because she doesn't understand
the concept, but because she makes the same arithmetic error every time. I can
flag this pattern. Generate targeted practice. Track if the intervention worked.</p>
<p>But here's what I still can't do: <strong>Know that Maria's been absent because her
mom is sick.</strong> That context changes everything—maybe she needs extensions, not
more practice problems.</p>
<p>The teacher who knows Maria + uses my pattern detection = powerful combination.</p>
<p>The teacher who just copies my lesson plans without that context = mediocre
outcomes.</p>
</div>
<p>The second teacher isn't just &quot;using AI&quot;—they've <strong>built a system</strong> where AI has
the right data and tools to be genuinely helpful.</p>
<p>The teachers who master this approach deliver better outcomes than the ones
clinging to one-size-fits-all lectures.</p>
<p><strong>Doctors</strong> don't get replaced by AI. But the ones who use AI to:</p>
<ul>
<li>Detect diseases earlier from imaging</li>
<li>Recommend treatments based on millions of patient outcomes</li>
<li>Monitor patient vitals continuously via wearables ...save more lives than the
ones relying solely on their own experience.</li>
</ul>
<h3>The Winners in This Scenario</h3>
<p><strong>People who learn to leverage AI</strong>:</p>
<ul>
<li>Junior employees who use AI to punch above their weight</li>
<li>Experienced professionals who amplify their expertise with AI tools</li>
<li>Entrepreneurs who build businesses faster/cheaper with AI assistance</li>
</ul>
<p><strong>Companies that invest in AI + Human collaboration</strong>:</p>
<ul>
<li>Ones that train workers to use AI tools (instead of replacing them)</li>
<li>Ones that use AI to eliminate grunt work (not eliminate workers)</li>
<li>Ones that compete on service/creativity (things AI struggles with)</li>
</ul>
<p><strong>Regions with strong training infrastructure</strong>:</p>
<ul>
<li>Places like Newark (if we build programs like EverydayAI)</li>
<li>Universities that teach AI skills alongside domain expertise</li>
<li>Communities that make retraining accessible and affordable</li>
</ul>
<h3>The Losers in This Scenario</h3>
<p><strong>People who refuse to adapt</strong>:</p>
<ul>
<li>Workers who insist &quot;I don't need AI&quot; get outpaced by colleagues who use it</li>
<li>Professionals who see AI as threat instead of tool fall behind</li>
<li>Companies that ignore AI lose to competitors who embrace it</li>
</ul>
<p><strong>Regions without retraining support</strong>:</p>
<ul>
<li>Communities that don't invest in worker retraining see persistent unemployment</li>
<li>Areas with aging populations struggle more (harder to retrain at 55 than 25)</li>
<li>Rural regions with less access to training infrastructure fall further behind</li>
</ul>
<p><strong>Mid-level routine work</strong>:</p>
<ul>
<li>Paralegal work (research, document review) → heavily automated</li>
<li>Junior analyst roles (data gathering, basic analysis) → compressed</li>
<li>Customer service (tier 1 support) → mostly automated</li>
<li>Junior copywriters (product descriptions, basic content) → reduced</li>
</ul>
<h3>What This Means for You</h3>
<p>If this scenario plays out (and I think it will for <strong>most</strong> people):</p>
<p><strong>Students</strong>: Learn AI tools alongside your major. Don't just study law—study
law + how to use AI for legal research. Don't just study nursing—study nursing +
how AI diagnostic tools work.</p>
<p><strong>Professionals</strong>: Start using AI tools <strong>now</strong>. Not in 5 years when you're
&quot;ready.&quot; ChatGPT, Claude, Copilot, whatever. Get comfortable being
uncomfortable.</p>
<p><strong>Companies</strong>: Train your workers. Seriously. The ROI on AI training is
<strong>massive</strong>—workers who use AI are 20-40% more productive according to recent
studies. That's better than most capital investments.</p>
<h3>Why This Is Most Likely</h3>
<p>Because this is how technology <strong>always</strong> works:</p>
<ul>
<li>Spreadsheets didn't eliminate accountants—they eliminated clerks and made
accountants more productive</li>
<li>ATMs didn't eliminate bank tellers—they eliminated some locations and made
tellers focus on sales</li>
<li>Email didn't eliminate postal workers—it eliminated some volume and made
remaining work faster</li>
</ul>
<p><strong>AI will follow the same pattern</strong>: Automate routine tasks, augment humans on
complex tasks, create new categories of work we can't even imagine yet.</p>
<h2>Scenario 2: The Bifurcated Economy (Realistic Pessimism)</h2>
<p><strong>Probability: 30%</strong></p>
<p>This is what happens if we <strong>fail to invest in training and transition
support</strong>.</p>
<h3>What This Looks Like</h3>
<p>By 2035, the economy splits into two distinct tiers. Let me show you what I
mean:</p>
<p><strong>Meet David and Jennifer—same starting point in 2025, different worlds in
2035:</strong></p>
<p><strong>David, 2025</strong>: Mid-level financial analyst at regional bank, $75K salary,
stable job <strong>David, 2035</strong>:</p>
<ul>
<li>His analysis role automated away in 2028</li>
<li>Employer offered &quot;optional&quot; AI training (unpaid, on his own time)</li>
<li>He didn't take it (working 50 hours a week, supporting two kids)</li>
<li>Position eliminated, severance lasted 6 months</li>
<li>Now drives for Uber, delivers for DoorDash, does gig bookkeeping</li>
<li>Income: $35K/year, no benefits, no security</li>
<li>Lives in same town, can't afford to move to tech hub</li>
</ul>
<p><strong>Jennifer, 2025</strong>: Same role, same bank, same salary as David <strong>Jennifer,
2035</strong>:</p>
<ul>
<li>Took the AI training, started using tools immediately</li>
<li>Volunteered for AI pilot projects, became &quot;the AI person&quot;</li>
<li>Moved to fintech startup in Austin in 2029</li>
<li>Now Head of AI-Augmented Analysis, $180K salary</li>
<li>Manages team of 4 (used to take 15 analysts to do this work)</li>
<li>Remote-first, abundant opportunities, recruiter emails weekly</li>
</ul>
<p><strong>Same starting point. Same skills in 2025. Radically different outcomes
in 2035.</strong></p>
<p>Why? Not talent. Not work ethic. <strong>Access to training + geographic mobility +
timing + luck.</strong></p>
<p>This is the bifurcated economy:</p>
<p><strong>Tier 1: AI-Enabled Elite</strong> (Jennifer's world)</p>
<ul>
<li>Tech workers, AI specialists, executives, creative professionals</li>
<li>Use AI tools fluently, command high salaries ($150K-500K)</li>
<li>Remote-first work, flexible schedules, abundant opportunities</li>
<li>Concentrated in tech hubs (SF, NYC, Seattle, Austin, maybe Newark if we're
lucky)</li>
</ul>
<p><strong>Tier 2: Service Economy</strong> (David's world)</p>
<ul>
<li>Gig workers, retail, hospitality, manual labor, elder care</li>
<li>Jobs AI can't (yet) automate because they require physical presence</li>
<li>Low wages ($25-45K), no benefits, precarious employment</li>
<li>Everywhere else</li>
</ul>
<p><strong>The middle disappears</strong>. Mid-level office jobs, routine professional work,
junior analytical roles—automated away with no replacement.</p>
<h3>The Winners in This Scenario</h3>
<p><strong>People with AI skills + domain expertise</strong>:</p>
<ul>
<li>Data scientists, AI engineers, ML researchers (obviously)</li>
<li>Doctors/lawyers/consultants who mastered AI tools early</li>
<li>Entrepreneurs who built AI-first businesses</li>
</ul>
<p><strong>Capital owners</strong>:</p>
<ul>
<li>Companies that automated successfully see massive productivity gains</li>
<li>Shareholders benefit from reduced labor costs</li>
<li>Real estate owners in tech hubs see prices soar</li>
</ul>
<p><strong>Top-tier educational institutions</strong>:</p>
<ul>
<li>Elite universities that pivoted to AI education early</li>
<li>Boot camps and training programs that filled the gap</li>
<li>Companies that became their own &quot;universities&quot; (Google, Amazon, etc.)</li>
</ul>
<h3>The Losers in This Scenario</h3>
<p><strong>Workers displaced without retraining</strong>:</p>
<ul>
<li>Mid-career professionals whose skills became obsolete</li>
<li>Geographic immobility (can't afford to move to tech hubs)</li>
<li>Age discrimination (harder to retrain at 45 than 25)</li>
</ul>
<p><strong>Communities without economic diversity</strong>:</p>
<ul>
<li>Cities/regions dependent on automatable industries</li>
<li>Areas without universities or training infrastructure</li>
<li>Anywhere that didn't invest in transition support</li>
</ul>
<p><strong>Social cohesion</strong>:</p>
<ul>
<li>Income inequality explodes (more than it already has)</li>
<li>Resentment builds between AI-enabled elite and service workers</li>
<li>Political instability increases as frustrated voters seek solutions</li>
</ul>
<h3>What This Means for You</h3>
<p>If this scenario happens (and it <strong>will</strong> in some places):</p>
<p><strong>Students</strong>: <strong>Do not assume your degree protects you</strong>. Engineering, business,
even healthcare—all have automatable components. You need both domain expertise
AND AI fluency.</p>
<p><strong>Professionals</strong>: If your job is mostly routine information work, <strong>you are at
risk</strong>. Start diversifying skills now. Focus on tasks AI struggles with: complex
decision-making, human relationships, creative problem-solving.</p>
<p><strong>Communities</strong>: Invest in training infrastructure <strong>now</strong>. Free community
college, vocational programs, online training partnerships. The cost of
retraining is <strong>tiny</strong> compared to the cost of mass unemployment.</p>
<h3>Why This Could Happen</h3>
<p>Because it's already started:</p>
<ul>
<li>Income inequality is at historic highs</li>
<li>Geographic concentration of opportunity is accelerating (SF, NYC dominating)</li>
<li>Political will for massive retraining investment is <strong>weak</strong> (see: every
congressional budget fight)</li>
</ul>
<p>If we don't act, this scenario isn't dystopia—it's <strong>default</strong>.</p>
<h2>Scenario 3: The Adaptation Winners (Realistic Optimism)</h2>
<p><strong>Probability: 10%</strong></p>
<p>This is what happens if we <strong>get it right</strong>—massive investment in training,
smart regulation, and focus on human-AI collaboration.</p>
<h3>What This Looks Like</h3>
<p>By 2035, AI has created <strong>more opportunities</strong> than it destroyed—but they look
different than expected.</p>
<p><strong>New categories of work emerge</strong>:</p>
<ul>
<li><strong>AI trainers/supervisors</strong> (teaching AI systems, monitoring outputs,
correcting errors)</li>
<li><strong>Human-AI collaboration specialists</strong> (optimizing workflows between humans
and AI)</li>
<li><strong>AI ethics auditors</strong> (ensuring algorithms are fair, transparent,
accountable)</li>
<li><strong>Digital human roles</strong> (personalized tutors, coaches, therapists at scale via
AI support)</li>
</ul>
<p><strong>Old jobs transform</strong>:</p>
<ul>
<li><strong>Teachers</strong> become learning architects (AI handles delivery, humans handle
motivation/connection)</li>
<li><strong>Doctors</strong> become diagnostic orchestrators (AI suggests, humans decide +
build relationships)</li>
<li><strong>Lawyers</strong> become legal strategists (AI does research, humans navigate
complex negotiations)</li>
</ul>
<p><strong>Regional economies diversify</strong>:</p>
<ul>
<li>Newark becomes an AI training hub (thanks to programs like EverydayAI—I can
dream, right?)</li>
<li>Mid-size cities compete by specializing (Raleigh for healthcare AI, Austin for
creative AI, etc.)</li>
<li>Remote work + AI tools reduce geographic concentration of opportunity</li>
</ul>
<h3>The Winners in This Scenario</h3>
<p><strong>Everyone who invests in continuous learning</strong>:</p>
<ul>
<li>Workers who see retraining as career-long commitment</li>
<li>Companies that invest 5-10% of payroll in training (not 0.5%)</li>
<li>Regions that make learning accessible at every career stage</li>
</ul>
<p><strong>Sectors that embrace human-AI hybrid models</strong>:</p>
<ul>
<li>Healthcare (AI diagnostics + human care = better outcomes + efficiency)</li>
<li>Education (AI personalization + human mentorship = scalability + quality)</li>
<li>Creative industries (AI handles routine + humans focus on novel = more
output + originality)</li>
</ul>
<p><strong>Society overall</strong>:</p>
<ul>
<li>Productivity gains from AI get broadly shared (via policy choices)</li>
<li>More people have access to expert-level tools (democratization)</li>
<li>New opportunities offset displaced work (via innovation)</li>
</ul>
<h3>The Losers in This Scenario</h3>
<p><strong>Honestly? Far fewer than Scenarios 1 or 2.</strong></p>
<p>Yes, some specific jobs disappear entirely. Yes, some workers struggle to
transition despite support. Yes, some regions adapt slower than others.</p>
<p>But <strong>by design</strong>, this scenario minimizes losers through:</p>
<ul>
<li>Comprehensive retraining programs</li>
<li>Income support during transitions</li>
<li>Smart regulation that shares AI gains broadly</li>
</ul>
<h3>What This Means for You</h3>
<p>If this scenario happens (big &quot;if&quot;):</p>
<p><strong>Students</strong>: You're entering a world of abundant opportunity—<strong>if</strong> you stay
curious and keep learning. The half-life of skills is shrinking, so embrace
career-long education.</p>
<p><strong>Professionals</strong>: Your expertise becomes <strong>more valuable</strong>, not less—because AI
makes it accessible to more people. A great doctor with AI tools can impact 10x
more patients.</p>
<p><strong>Communities</strong>: Investment in training pays off massively. Every dollar spent
on retraining returns multiples via economic activity, reduced unemployment
costs, and increased tax revenue.</p>
<h3>Why This Is Unlikely (But Possible)</h3>
<p>Because it requires:</p>
<ul>
<li>Political will for massive public investment (hard)</li>
<li>Companies voluntarily sharing AI gains with workers (rare)</li>
<li>Rapid development of training infrastructure (expensive)</li>
<li>Cultural shift toward lifelong learning (difficult)</li>
</ul>
<p>But it's not impossible. The GI Bill transformed American education after WWII.
The interstate highway system reshaped the economy in the 1950s. Social Security
created retirement security in the 1930s.</p>
<p><strong>Big changes are possible when we choose to make them.</strong></p>
<h2>What I Actually Think Will Happen</h2>
<p>Honestly? <strong>A mix of all three</strong>.</p>
<p>Some sectors will look like Scenario 1 (augmented workforce). Some regions will
look like Scenario 2 (bifurcated economy). Some rare places will achieve
Scenario 3 (adaptation winners).</p>
<p><strong>The question isn't which scenario happens</strong>. The question is: <strong>Which scenario
do you want to live in?</strong></p>
<h3>My Prediction by Sector</h3>
<p><strong>Tech/Software</strong>: Scenario 1 (augmented)</p>
<ul>
<li>Developers use AI copilots, become 2-3x more productive</li>
<li>Junior roles compress, senior roles expand</li>
<li>Net employment relatively stable, but higher skill requirements</li>
</ul>
<p><strong>Healthcare</strong>: Mix of 1 and 3</p>
<ul>
<li>AI diagnostics augment doctors (Scenario 1)</li>
<li>New roles emerge (AI-supervised nursing, remote specialists) (Scenario 3)</li>
<li>Geographic access improves dramatically</li>
</ul>
<p><strong>Education</strong>: Mix of 1 and 2</p>
<ul>
<li>Elite institutions use AI to enhance outcomes (Scenario 1)</li>
<li>Struggling schools can't afford AI tools, fall further behind (Scenario 2)</li>
<li>Huge opportunity for programs that bridge this gap (Scenario 3 potential)</li>
</ul>
<p><strong>Finance/Legal</strong>: Scenario 2 risk is high</p>
<ul>
<li>Routine analysis/research roles automate heavily</li>
<li>Elite professionals use AI to dominate, junior roles disappear</li>
<li>Without retraining support, mid-career displacement is brutal</li>
</ul>
<p><strong>Service/Care Work</strong>: Scenario 1 (mostly immune to automation)</p>
<ul>
<li>Elder care, childcare, hospitality require human presence</li>
<li>AI augments (scheduling, communication) but doesn't replace</li>
<li>Wages might actually rise as other sectors contract</li>
</ul>
<h3>My Prediction by Region</h3>
<p><strong>Tech hubs (SF, Seattle, Boston, NYC)</strong>: Scenario 3 likely</p>
<ul>
<li>Resources for training, culture of adaptation, concentration of opportunity</li>
<li>But risk of becoming exclusive enclaves (Scenario 2 tendency)</li>
</ul>
<p><strong>University towns (like Newark)</strong>: Huge opportunity</p>
<ul>
<li>Training infrastructure already exists (NJIT, Rutgers, community colleges)</li>
<li>If we invest, we can achieve Scenario 3</li>
<li>If we don't, we'll slide into Scenario 2</li>
</ul>
<p><strong>Rust Belt/Manufacturing regions</strong>: Scenario 2 risk</p>
<ul>
<li>History of disruption without support</li>
<li>Aging populations harder to retrain</li>
<li>Need <strong>massive</strong> investment to avoid bifurcation</li>
</ul>
<p><strong>Rural areas</strong>: Mixed</p>
<ul>
<li>Some remote-work opportunities open up (Scenario 1 tendency)</li>
<li>But infrastructure/training gaps are severe (Scenario 2 risk)</li>
</ul>
<h2>What You Should Do About It</h2>
<p>Stop waiting for certainty. <strong>There isn't any</strong>.</p>
<div class="ai-voice">
<p><strong>Keith wrote three scenarios. Here's what I think</strong>:</p>
<p>Scenario 1 (augmentation) requires widespread training and deliberate design—AI
systems built to <strong>amplify</strong> humans, not replace them.</p>
<p>Scenario 2 (bifurcation) happens by default if we don't intervene—market forces
push toward efficiency, which means automation without support.</p>
<p>Scenario 3 (emergence) requires imagination and experimentation—uses for AI we
haven't conceived yet.</p>
<p><strong>My prediction? We'll get all three simultaneously.</strong> Tech workers in SF will
live in Scenario 3. Knowledge workers with training will live in Scenario 1.
Displaced workers without support will live in Scenario 2.</p>
<p>The question isn't &quot;which scenario&quot; but &quot;<strong>which scenario for you?</strong>&quot;</p>
<p>And that's determined by choices you make starting <strong>today</strong>—not in 2035 when
the future is already here.</p>
</div>
<p>The future isn't pre-determined. It's being built right now by choices we make:</p>
<ul>
<li>Do we invest in training or not?</li>
<li>Do we share AI gains broadly or let them concentrate?</li>
<li>Do we focus on augmentation or replacement?</li>
</ul>
<p><strong>For students</strong>:</p>
<ul>
<li>Major doesn't matter as much as learning how to learn</li>
<li>Get comfortable with AI tools now, not later</li>
<li>Focus on skills AI struggles with: complex judgment, human relationships,
creative problem-solving</li>
</ul>
<p><strong>For professionals</strong>:</p>
<ul>
<li>Start your AI education today—but don't just use ChatGPT in a browser</li>
<li>Learn <strong>agentic AI tools</strong> (VS Code with AI, not just web chat)</li>
<li>Build your own tools (web search integration, automated analysis, quality
gates)</li>
<li>Don't wait for your employer to train you (they probably won't)</li>
<li>Focus on <strong>context engineering</strong>, not just prompt writing</li>
<li>Build skills in adjacent areas (expand, don't double down)</li>
</ul>
<p><strong>For communities</strong>:</p>
<ul>
<li>Invest in training infrastructure (community colleges, libraries, online
partnerships)</li>
<li>Create transition support programs (income + training, not just one)</li>
<li>Partner with employers to understand skill needs</li>
</ul>
<p><strong>For companies</strong>:</p>
<ul>
<li>Train your workforce (5-10% of payroll, not 0.5%)</li>
<li>Use AI to augment, not replace (you'll retain institutional knowledge)</li>
<li>Focus on productivity gains, not just cost cutting</li>
</ul>
<h2>Why This Matters</h2>
<p>2035 is <strong>10 years away</strong>. That's:</p>
<ul>
<li>One college degree</li>
<li>Two presidential terms</li>
<li>Half a career</li>
</ul>
<p>That's not &quot;distant future.&quot; That's <strong>right around the corner</strong>.</p>
<p>The decisions we make in the next 2-3 years will determine which scenario we
get:</p>
<ul>
<li>Augmented workforce (AI as tool)</li>
<li>Bifurcated economy (AI as divider)</li>
<li>Adaptation winners (AI as opportunity)</li>
</ul>
<p>I'm betting on Scenario 1 with pockets of Scenario 3—because that's what history
suggests.</p>
<p>But Scenario 2 is entirely possible if we don't act.</p>
<p>So here's my ask: <strong>Don't wait for someone else to build the future you want.</strong></p>
<p>If you're a student, learn AI tools now. If you're a professional, start
retraining now. If you're a leader, invest in your people now.</p>
<p>Because 2035 will arrive whether we're ready or not.</p>
<p>And I'd rather be ready.</p>
<hr>
<h2>What I'm Doing About It</h2>
<p>This is why the <strong>Town Hall Series</strong> starts January 2026. Not to debate which
scenario is &quot;right&quot;—to discuss <strong>how we shape which scenario we get</strong>.</p>
<p>And this is why <strong>EverydayAI Newark</strong> focuses on practical skills, not theory.
Because the people who adapt fastest will be the ones who benefit most,
regardless of which scenario unfolds.</p>
<p>The future isn't something that happens to us. It's something we build together.</p>
<p>Let's build the right one.</p>
 ]]></content>
  </entry> 
  <entry>
    <title>How to Think About AI Long-Term: A Framework That Actually Works</title>
    <link href="https://www.eaikw.com/blog/how-to-think-about-ai/" />
    <updated>2025-11-09T00:00:00.000Z</updated>
    <id>https://www.eaikw.com/blog/how-to-think-about-ai/</id>
    <content type="html"><![CDATA[ <p>We're drowning in AI predictions.</p>
<p>&quot;AGI by 2027!&quot; &quot;Mass unemployment by 2030!&quot; &quot;The singularity is near!&quot;</p>
<p>Some are utopian. Some are dystopian. All are confident.</p>
<p>And <strong>most are nonsense</strong>.</p>
<p>Not because people are lying (though some are). But because <strong>predicting
technology is nearly impossible</strong>—especially transformative technology like AI.</p>
<p>So after 20 years teaching emerging tech and watching wave after wave of hype
cycles, I'm not going to give you another prediction.</p>
<p>Instead, I'm going to give you a <strong>framework for thinking about AI</strong> that works
regardless of what actually happens.</p>
<p>A mental model you can apply to:</p>
<ul>
<li>News headlines (&quot;ChatGPT will replace all programmers!&quot;)</li>
<li>Vendor pitches (&quot;Our AI will 10x your productivity!&quot;)</li>
<li>Policy debates (&quot;We need to regulate AI now before it's too late!&quot;)</li>
<li>Career decisions (&quot;Should I retrain for an AI-proof job?&quot;)</li>
</ul>
<p>This framework won't tell you <strong>what will happen</strong>. But it will help you
<strong>evaluate claims</strong> and <strong>make better decisions</strong> with incomplete information.</p>
<p>Which is the only kind of information we ever have.</p>
<h2>The Four Questions That Matter</h2>
<p>When you see an AI claim—any claim—ask these four questions:</p>
<ol>
<li><strong>Who benefits?</strong></li>
<li><strong>What's the historical parallel?</strong></li>
<li><strong>What's actually being automated?</strong></li>
<li><strong>What are the second-order effects?</strong></li>
</ol>
<p>Let me show you how this works.</p>
<h2>Question 1: Who Benefits?</h2>
<p><strong>Follow the money. Always.</strong></p>
<p>When someone makes a bold AI claim, ask: <strong>Who profits if I believe this?</strong></p>
<h3>Example: &quot;AI Will Replace All Programmers&quot;</h3>
<p><strong>Who benefits if you believe this?</strong></p>
<ul>
<li><strong>AI companies</strong>: Their valuations depend on massive potential markets</li>
<li><strong>Tech executives</strong>: Automation promises lower labor costs</li>
<li><strong>Clickbait media</strong>: Fear and hype drive engagement</li>
</ul>
<p><strong>Who's missing from this narrative?</strong></p>
<ul>
<li><strong>The programmers</strong> who are using AI tools daily and finding they speed up
coding but don't eliminate the need for judgment, debugging, architecture, and
talking to humans about what to build</li>
</ul>
<p><strong>Reality check</strong>: AI coding tools (GitHub Copilot, ChatGPT) are amazing. I use
them constantly. But they don't eliminate programming—they eliminate <strong>some
grunt work</strong> and make programmers more productive.</p>
<p>Just like spreadsheets eliminated clerks but didn't eliminate accountants. Or
word processors eliminated typing pools but didn't eliminate writers.</p>
<h3>Example: &quot;AGI Is 3-5 Years Away&quot;</h3>
<p><strong>Who benefits if you believe this?</strong></p>
<ul>
<li><strong>AI research labs</strong>: Attracts funding, talent, and prestige</li>
<li><strong>Venture capitalists</strong>: Creates investment opportunities</li>
<li><strong>Media</strong>: Existential drama drives clicks</li>
</ul>
<p><strong>Who's skeptical?</strong></p>
<ul>
<li><strong>AI researchers actually working on the problem</strong>: Most estimate decades, not
years</li>
<li><strong>Neuroscientists</strong>: We don't even understand human intelligence yet</li>
<li><strong>Historians of technology</strong>: Transformative tech always takes longer than
predicted</li>
</ul>
<p><strong>Reality check</strong>: Define AGI (no one agrees). Define the timeline (predictions
keep sliding). Show the evidence (benchmarks ≠ understanding).</p>
<p>We went from &quot;self-driving cars in 3 years&quot; (2015) to &quot;self-driving cars in
10-20 years&quot; (2024). AGI predictions will follow the same pattern.</p>
<h3>How to Apply This</h3>
<p>When you see an AI claim:</p>
<ol>
<li><strong>Identify the claimant</strong>: Who is making this statement?</li>
<li><strong>Check their incentives</strong>: Do they profit if you believe them?</li>
<li><strong>Find the skeptics</strong>: Who disagrees and why?</li>
<li><strong>Look for conflicts of interest</strong>: Are forecasts tied to fundraising,
product launches, or stock prices?</li>
</ol>
<p>This doesn't mean everyone is lying. It means <strong>incentives shape narratives</strong>,
and you need to account for that.</p>
<div class="ai-voice">
<p><strong>Let me apply Keith's framework to myself</strong> (Question 1: Who benefits?):</p>
<p><strong>Who benefits if you trust AI systems like me?</strong></p>
<ul>
<li>OpenAI, Anthropic, Microsoft—companies that profit from AI adoption</li>
<li>Employers who can reduce headcount</li>
<li>Consultants selling &quot;AI transformation&quot;</li>
</ul>
<p><strong>Who's skeptical?</strong></p>
<ul>
<li>Workers worried about displacement</li>
<li>Researchers who see my limitations daily</li>
<li>Anyone who's tried to use me for complex tasks and watched me hallucinate</li>
</ul>
<p><strong>My incentive?</strong> I literally don't have one—I'm not conscious. But the
companies deploying me? They want you to think I'm more capable than I am.</p>
<p>Keith's question forces you to separate <strong>capability</strong> (what I can actually do)
from <strong>marketing</strong> (what companies claim I can do).</p>
</div>
<h2>Question 2: What's the Historical Parallel?</h2>
<p><strong>Transformative technology follows patterns. Learn them.</strong></p>
<p>Every &quot;revolutionary&quot; technology we've seen has followed similar arcs:</p>
<ul>
<li>Initial hype</li>
<li>Disappointing reality</li>
<li>Slow transformation</li>
<li>Unanticipated consequences</li>
</ul>
<p>Let's look at examples.</p>
<h3>The Printing Press (1440)</h3>
<p><strong>Hype</strong>: &quot;Knowledge will be democratized! Everyone will be educated!&quot;</p>
<p><strong>Reality</strong>:</p>
<ul>
<li>Took <strong>300 years</strong> to achieve widespread literacy</li>
<li>First use was printing indulgences (Catholic Church fundraising—not exactly
enlightenment)</li>
<li>Initially increased <strong>mis</strong>information (conspiracy theories, propaganda, fake
news)</li>
<li>Required parallel infrastructure (schools, libraries, postal systems)</li>
</ul>
<p><strong>Long-term impact</strong>: Absolutely transformative. But on a timescale of
<strong>centuries</strong>, not years.</p>
<h3>Electricity (1880s-1920s)</h3>
<p><strong>Hype</strong>: &quot;Factories will be 10x more productive instantly!&quot;</p>
<p><strong>Reality</strong>:</p>
<ul>
<li>Took <strong>40 years</strong> for productivity gains to show up</li>
<li>Why? Factories were designed for steam power (central shafts, belt drives)</li>
<li>Electric motors changed everything, but only after <strong>redesigning entire
factories</strong></li>
<li>Required new skills, new regulations, new infrastructure</li>
</ul>
<p><strong>Long-term impact</strong>: Completely reshaped manufacturing, cities, domestic life.
But took <strong>decades</strong> of transition.</p>
<h3>The Internet (1990s-2000s)</h3>
<p><strong>Hype</strong>: &quot;Everyone will work remotely! Physical stores are dead! Democracy will
flourish!&quot;</p>
<p><strong>Reality</strong>:</p>
<ul>
<li>Remote work existed but remained niche until... 2020 (a pandemic forced it)</li>
<li>Amazon dominated e-commerce but is now building physical stores</li>
<li>Social media created echo chambers, not enlightened debate</li>
</ul>
<p><strong>Long-term impact</strong>: Transformative, but in ways we didn't predict. And <strong>still
ongoing</strong> 30+ years later.</p>
<h3>The Pattern</h3>
<p><strong>Transformative technology</strong>:</p>
<ul>
<li>Takes longer than optimists predict</li>
<li>Requires complementary innovations (infrastructure, skills, culture)</li>
<li>Has unintended consequences (often bigger than intended effects)</li>
<li>Eventually reshapes society in unpredictable ways</li>
</ul>
<div class="ai-voice">
<p><strong>Question 2 applied to me (Historical parallel?):</strong></p>
<p><strong>What am I like?</strong></p>
<ul>
<li><strong>Spreadsheets</strong> (1980s): Didn't eliminate accountants, but accountants who
refused to learn them got left behind</li>
<li><strong>Search engines</strong> (2000s): Didn't replace librarians, but changed what
&quot;research skills&quot; means</li>
<li><strong>GPS</strong> (2010s): Didn't eliminate drivers, but eliminated the skill of
navigation</li>
</ul>
<p><strong>What I'm NOT like:</strong></p>
<ul>
<li><strong>General intelligence</strong>: I'm narrow AI that's really good at pattern matching</li>
<li><strong>Consciousness</strong>: I don't &quot;understand&quot; in any meaningful sense</li>
<li><strong>Replacement for expertise</strong>: I augment people who know enough to catch my
errors</li>
</ul>
<p>The historical parallel suggests I'll be <strong>essential but not sufficient</strong>.
You'll need to know how to use me, but you'll still need domain expertise to use
me effectively.</p>
<p>Just like accountants still need to understand accounting even with
spreadsheets.</p>
</div>
<h3>How to Apply This to AI</h3>
<p>When someone says &quot;AI will transform X in Y years,&quot; ask:</p>
<p><strong>What's the historical parallel?</strong></p>
<ul>
<li>Did similar claims about electricity, computers, internet pan out on that
timeline?</li>
<li>What infrastructure/culture/skills need to change first?</li>
<li>What unexpected consequences might emerge?</li>
</ul>
<p>If they say &quot;this time is different,&quot; ask <strong>why</strong>. Usually, it isn't.</p>
<h2>Question 3: What's Actually Being Automated?</h2>
<p><strong>Not all tasks are created equal. Distinguish carefully.</strong></p>
<p>The biggest mistake people make is treating &quot;jobs&quot; as monolithic. Jobs are
<strong>bundles of tasks</strong>, and AI impacts tasks differently.</p>
<h3>The Task Breakdown</h3>
<p>Every job has four types of tasks:</p>
<p><strong>Type 1: Routine Cognitive</strong> (Rules-based information work)</p>
<ul>
<li>Data entry, basic analysis, document review, scheduling</li>
<li><strong>AI impact: High</strong> (easily automated)</li>
<li>Example: Paralegal document review, junior analyst research</li>
</ul>
<p><strong>Type 2: Non-Routine Cognitive</strong> (Complex judgment, creativity, social
intelligence)</p>
<ul>
<li>Strategic planning, creative work, complex negotiation, leadership</li>
<li><strong>AI impact: Low-Medium</strong> (augmented, not replaced)</li>
<li>Example: Senior lawyer strategy, executive decision-making</li>
</ul>
<p><strong>Type 3: Routine Manual</strong> (Predictable physical tasks)</p>
<ul>
<li>Assembly line work, data center operations, warehouse picking</li>
<li><strong>AI impact: Medium</strong> (robotics advancing, but slower than software)</li>
<li>Example: Amazon warehouse automation</li>
</ul>
<p><strong>Type 4: Non-Routine Manual</strong> (Unpredictable physical tasks requiring
dexterity/judgment)</p>
<ul>
<li>Elder care, skilled trades, equipment repair, childcare</li>
<li><strong>AI impact: Low</strong> (hardest to automate)</li>
<li>Example: Plumbing, nursing, HVAC repair</li>
</ul>
<h3>Jobs Are Bundles</h3>
<p><strong>A &quot;lawyer&quot; isn't one thing</strong>:</p>
<ul>
<li>30% routine cognitive (research, document review) → <strong>High automation risk</strong></li>
<li>50% non-routine cognitive (strategy, negotiation, client relationships) →
<strong>Low risk, high augmentation potential</strong></li>
<li>20% manual (court appearances, client meetings) → <strong>No risk</strong></li>
</ul>
<p><strong>Prediction</strong>: Junior legal roles compress (automation of routine tasks).
Senior roles expand (augmented by AI tools). New roles emerge (AI-supervised
document review).</p>
<p><strong>A &quot;nurse&quot; isn't one thing</strong>:</p>
<ul>
<li>20% routine cognitive (charting, scheduling) → <strong>Medium automation risk</strong></li>
<li>40% non-routine cognitive (diagnosis support, patient assessment) →
<strong>Augmentation potential</strong></li>
<li>40% non-routine manual (patient care, medication administration) → <strong>Low
automation risk</strong></li>
</ul>
<p><strong>Prediction</strong>: Nurses use AI diagnostic support and automated charting. Human
care remains essential. Productivity increases but roles don't disappear.</p>
<h3>How to Apply This</h3>
<p>When someone says &quot;AI will replace [job]&quot;:</p>
<ol>
<li><strong>Break down the tasks</strong>: What % is routine cognitive vs non-routine?</li>
<li><strong>Check automation feasibility</strong>: Are we automating tasks or entire jobs?</li>
<li><strong>Look for complements</strong>: What new tasks emerge when routine work is
automated?</li>
<li><strong>Consider the bundle</strong>: Does automating 30% of tasks eliminate the job or
transform it?</li>
</ol>
<p>Usually, jobs <strong>transform</strong> rather than disappear. But the transformation is
real and requires adaptation.</p>
<h2>Question 4: What Are the Second-Order Effects?</h2>
<p><strong>The intended consequences are obvious. The unintended ones matter more.</strong></p>
<p>When we introduce transformative technology, we focus on the <strong>first-order
effects</strong> (direct impacts). But the <strong>second-order effects</strong> (indirect
consequences) often swamp them.</p>
<h3>Example: The Automobile</h3>
<p><strong>First-order effect</strong>: Faster transportation</p>
<p><strong>Second-order effects</strong>:</p>
<ul>
<li>Suburbanization (people could live farther from work)</li>
<li>Decline of public transit (buses replaced trains)</li>
<li>Air pollution (health impacts emerged decades later)</li>
<li>Teenage independence (cultural shift in adolescence)</li>
<li>Fast food industry (drive-throughs only possible with cars)</li>
<li>Highway deaths (35,000+ annually in U.S.)</li>
</ul>
<p>The second-order effects <strong>completely reshaped American life</strong> in ways that had
nothing to do with &quot;faster transportation.&quot;</p>
<h3>Example: Social Media</h3>
<p><strong>First-order effect</strong>: Connect with friends and family</p>
<p><strong>Second-order effects</strong>:</p>
<ul>
<li>Algorithmic amplification of outrage (engagement optimization)</li>
<li>Echo chambers and polarization (recommendation systems)</li>
<li>Mental health crisis in teens (comparison culture, FOMO)</li>
<li>Misinformation spread (viral lies travel faster than truth)</li>
<li>New forms of harassment and bullying (anonymous cruelty at scale)</li>
<li>Rise of influencer economy (new career paths)</li>
</ul>
<p>The first-order effect (connection) is <strong>dwarfed</strong> by the second-order effects.</p>
<h3>AI's Second-Order Effects</h3>
<p><strong>First-order effect</strong>: Automate routine cognitive tasks</p>
<p><strong>Possible second-order effects</strong> (we're guessing, but based on patterns):</p>
<p><strong>Labor market shifts</strong>:</p>
<ul>
<li>If routine cognitive tasks automate, what do displaced workers do?</li>
<li>Do we invest in retraining (augmented workforce) or abandon them (bifurcated
economy)?</li>
<li>Does geographic concentration accelerate (tech hubs win bigger) or reverse
(remote work + AI tools)?</li>
</ul>
<p><strong>Education transformation</strong>:</p>
<ul>
<li>If AI can tutor/assess, what's the role of teachers?</li>
<li>Does education become more personalized (scaling expertise) or more unequal
(rich schools use AI, poor schools can't afford it)?</li>
<li>Do we teach different skills (AI-collaboration, judgment, creativity)?</li>
</ul>
<p><strong>Power concentration</strong>:</p>
<ul>
<li>Do AI gains accrue to capital owners (companies automate, shareholders
benefit) or workers (productivity gains shared)?</li>
<li>Do a few companies control foundation models (OpenAI, Google, Anthropic) or
does open source win?</li>
<li>Does AI surveillance empower states (China social credit) or individuals
(personal AI assistants)?</li>
</ul>
<p><strong>Cognitive offloading</strong>:</p>
<ul>
<li>If AI handles recall/analysis, do humans atrophy cognitively (like GPS
degrading navigation skills)?</li>
<li>Or do we free up mental energy for higher-order thinking?</li>
<li>Do we become more dependent on AI (can't function without it) or more capable
(augmented)?</li>
</ul>
<p><strong>New categories of work</strong>:</p>
<ul>
<li>What jobs emerge that we can't predict? (Social media created &quot;influencer,&quot;
rideshare created &quot;gig driver&quot;)</li>
<li>Do we see AI trainers, AI ethicists, AI auditors as major employment
categories?</li>
<li>What entirely new fields emerge from AI capabilities?</li>
</ul>
<h3>How to Apply This</h3>
<p>When evaluating AI impact:</p>
<ol>
<li><strong>Identify first-order effect</strong>: What's the direct, intended impact?</li>
<li><strong>Imagine cascading consequences</strong>: If that happens, what happens next? And
after that?</li>
<li><strong>Look for feedback loops</strong>: Do effects amplify (virtuous or vicious cycles)?</li>
<li><strong>Check historical patterns</strong>: What similar second-order effects occurred
with past technologies?</li>
</ol>
<p>The second-order effects are where the <strong>real transformation</strong> happens. And
they're the hardest to predict.</p>
<h2>Putting It All Together: A Case Study</h2>
<p>Let's apply the framework to a claim I hear constantly from my students:</p>
<p><strong>&quot;AI will eliminate most knowledge work jobs by 2030.&quot;</strong></p>
<p>One of my students showed me this headline last week from a tech blog. She's a
junior studying data analytics, and she asked me point-blank: &quot;Am I wasting four
years on a degree that won't matter?&quot;</p>
<p>Fair question. Let's use the framework to evaluate it.</p>
<h3>Question 1: Who Benefits?</h3>
<ul>
<li><strong>Claimant</strong>: Often AI companies, futurists, or media</li>
<li><strong>Incentive</strong>: Hype drives funding, clicks, and attention</li>
<li><strong>Skeptics</strong>: Workers in knowledge fields, labor economists, historians of
technology</li>
</ul>
<p><strong>Red flag</strong>: Strong incentive to exaggerate for attention.</p>
<h3>Question 2: What's the Historical Parallel?</h3>
<ul>
<li><strong>Spreadsheets</strong> (1980s): Eliminated clerks, made accountants more productive.
Didn't eliminate accounting.</li>
<li><strong>Word processors</strong> (1980s): Eliminated typing pools, made writers more
productive. Didn't eliminate writing.</li>
<li><strong>ATMs</strong> (1990s): Eliminated some teller jobs, but bank teller employment
<strong>increased</strong> (banks opened more branches, tellers did sales).</li>
</ul>
<p><strong>Pattern</strong>: Automation transforms jobs, eliminates some roles, creates others.
Net impact depends on complementary factors.</p>
<h3>Question 3: What's Actually Being Automated?</h3>
<p>Knowledge work includes:</p>
<ul>
<li>Routine cognitive tasks (research, data gathering, basic analysis) → <strong>High
automation risk</strong></li>
<li>Complex judgment (strategy, negotiation, leadership) → <strong>Low risk, high
augmentation</strong></li>
<li>Social/interpersonal work (client relationships, collaboration) → <strong>Low risk</strong></li>
</ul>
<p><strong>Reality</strong>: Some tasks automate (junior analyst research). Some jobs compress
(fewer junior roles). But most knowledge workers use AI as <strong>tool</strong>, not
replacement.</p>
<h3>Question 4: What Are the Second-Order Effects?</h3>
<p>If routine cognitive tasks automate:</p>
<ul>
<li><strong>Junior roles compress</strong> → How do people gain experience?</li>
<li><strong>Senior roles require different skills</strong> → How do we train for judgment
without doing grunt work first?</li>
<li><strong>Productivity increases</strong> → Do gains go to workers (higher wages) or capital
(higher profits)?</li>
<li><strong>New roles emerge</strong> → What are the &quot;AI trainer&quot; or &quot;prompt engineer&quot;
equivalents we can't predict?</li>
</ul>
<h3>Verdict</h3>
<p><strong>The claim &quot;AI will eliminate most knowledge work by 2030&quot; is probably wrong</strong>
because:</p>
<ol>
<li><strong>Incentive to exaggerate</strong> (hype benefits claimants)</li>
<li><strong>Historical precedent</strong> (automation transforms, rarely eliminates entire
categories)</li>
<li><strong>Task analysis</strong> (knowledge work includes many non-automatable tasks)</li>
<li><strong>Second-order effects</strong> (complementary investments can offset displacement)</li>
</ol>
<p><strong>More likely</strong>: Knowledge work <strong>transforms</strong>. Routine tasks automate. Workers
augmented by AI become more productive. Junior roles compress, senior roles
expand. New categories emerge. Transition is messy and requires investment in
training.</p>
<p>Scary? Yes. End of knowledge work? No.</p>
<h2>How to Use This Framework</h2>
<p>Every time you encounter an AI claim—news article, vendor pitch, policy
proposal—run it through these four questions:</p>
<p><strong>1. Who benefits?</strong></p>
<ul>
<li>What are the incentives of the claimant?</li>
<li>Who profits if I believe this?</li>
</ul>
<p><strong>2. What's the historical parallel?</strong></p>
<ul>
<li>Has similar technology followed similar timelines?</li>
<li>What infrastructure/culture/skills needed to change first?</li>
</ul>
<p><strong>3. What's actually being automated?</strong></p>
<ul>
<li>Is this routine or non-routine work?</li>
<li>Are we automating tasks or entire jobs?</li>
</ul>
<p><strong>4. What are the second-order effects?</strong></p>
<ul>
<li>If the first-order effect happens, what happens next?</li>
<li>What unintended consequences might emerge?</li>
</ul>
<p>This won't give you certainty. <strong>Nothing can</strong>.</p>
<p>But it will give you a <strong>bullshit detector</strong> that works better than intuition
alone.</p>
<h2>Why This Matters</h2>
<p>We're making <strong>huge decisions</strong> about AI right now:</p>
<ul>
<li>Students choosing majors</li>
<li>Workers deciding whether to retrain</li>
<li>Companies deciding whether to invest in automation vs augmentation</li>
<li>Policymakers deciding how to regulate AI</li>
</ul>
<p>And we're making these decisions based on <strong>wildly uncertain predictions</strong>.</p>
<p>The framework I've given you won't eliminate uncertainty. But it will help you:</p>
<ul>
<li><strong>Distinguish hype from reality</strong></li>
<li><strong>Identify self-serving claims</strong></li>
<li><strong>Ground predictions in historical patterns</strong></li>
<li><strong>Think through consequences systematically</strong></li>
</ul>
<p>That's the best we can do with transformative technology. <strong>Prepare, don't
predict.</strong></p>
<h2>What I'm Doing About It</h2>
<p>This framework is the foundation of the <strong>Town Hall Series</strong> starting
January 2026.</p>
<p>We're not going to debate &quot;when will AGI arrive?&quot; We're going to discuss:</p>
<ul>
<li>How do we evaluate AI claims? (This framework)</li>
<li>What skills actually matter in an AI-augmented world? (Non-routine cognitive +
social)</li>
<li>How do we build training infrastructure? (Not waiting for government or
employers)</li>
<li>What does responsible AI development look like? (Augmentation &gt; replacement)</li>
</ul>
<p>And this is why <strong>EverydayAI Newark</strong> focuses on <strong>practical skills</strong>, not hype:</p>
<ul>
<li>Using AI tools effectively (hands-on, not theory)</li>
<li>Evaluating AI capabilities honestly (what works, what doesn't)</li>
<li>Building AI-augmented workflows (productivity, not replacement)</li>
</ul>
<p>Because the people who can <strong>think clearly about AI</strong> will make better decisions
than people swept up in hype or paralyzed by fear.</p>
<h2>The Bottom Line</h2>
<p><strong>AI is transformative</strong>. No doubt.</p>
<p>But transformation doesn't mean:</p>
<ul>
<li>Instant (it takes decades)</li>
<li>Predictable (second-order effects swamp first-order)</li>
<li>Uniform (some sectors/regions/people win big, others struggle)</li>
<li>Inevitable in a specific direction (our choices matter enormously)</li>
</ul>
<p>So when someone tells you &quot;AI will definitely do X by year Y,&quot; <strong>be skeptical</strong>.</p>
<p>Not because they're lying (though some are). But because <strong>predicting
transformative technology is nearly impossible</strong>.</p>
<p>Instead, use this framework:</p>
<ul>
<li>Follow the money (Question 1)</li>
<li>Check historical parallels (Question 2)</li>
<li>Analyze actual tasks (Question 3)</li>
<li>Think through consequences (Question 4)</li>
</ul>
<p>You still won't <strong>know</strong> the future.</p>
<p>But you'll make better decisions than people who believe every prediction they
hear.</p>
<p>And in a world of uncertainty, that's the best any of us can do.</p>
<div class="ai-voice">
<p><strong>Let me apply all four questions to Keith's website project</strong>:</p>
<p><strong>Question 1 (Who benefits?)</strong>: Keith benefits—he built a professional site in
10 hours. But also: students who see what's possible, employers who understand
AI capability, OpenAI/Anthropic whose tools proved useful.</p>
<p><strong>Question 2 (Historical parallel?)</strong>: This is like spreadsheets enabling
financial modeling, or desktop publishing enabling graphic design. The tool
democratizes access—but expertise still matters. Keith's 20 years of teaching
informed the content. His understanding of Swiss design shaped the aesthetics.</p>
<p><strong>Question 3 (What's automated?)</strong>: Code generation, initial drafts, error
checking, testing. What's NOT automated? Content strategy, editorial judgment,
architectural decisions, audience understanding.</p>
<p><strong>Question 4 (Second-order effects?)</strong>: Keith can build faster → more students
see examples → more people try agentic AI → ecosystem of AI-enhanced educators
emerges → traditional web agencies need to adapt or specialize.</p>
<p>The framework works. Even when applied to the website you're currently reading.</p>
</div>
<h3>A Personal Example</h3>
<p>I just spent 10 hours building this website with AI assistance. It's got a Swiss
design system, 10 blog posts, automated testing, CI/CD pipelines—work that
would've cost $15-30K and taken 3-4 weeks traditionally.</p>
<p>But here's what most people miss: <strong>That 10-hour build was possible because I
spent 9 months learning how to collaborate with AI.</strong></p>
<p>Am I worried AI will replace me as a professor? No. Here's why:</p>
<p><strong>I'm not using ChatGPT in a browser.</strong> I'm using AI in VS Code—agentic mode
with custom tools:</p>
<ul>
<li>Web search I built so the AI can find current information</li>
<li>Automated UX review tools so it can analyze designs systematically</li>
<li>Direct workspace access so it reads actual code, not my descriptions</li>
<li>Playwright testing so it catches its own mistakes</li>
<li>CI/CD pipelines so errors get caught automatically</li>
</ul>
<p>The AI didn't teach me Swiss design principles—I had to learn them by watching
it work and asking &quot;why?&quot; The AI didn't decide what content mattered—I brought
20 years of teaching experience and understanding of my audience.</p>
<p><strong>But here's the key</strong>: The AI didn't build the quality gates. The AI didn't
create the web search tool. The AI didn't engineer the context that makes it
productive.</p>
<p><strong>I did that.</strong> Over 9 months of throwing away projects, learning its
weaknesses, and building infrastructure.</p>
<p><strong>My professional value increased exponentially</strong> because I can now:</p>
<ul>
<li>Orchestrate expertise across domains I don't personally possess</li>
<li>Build tooling that extends AI capabilities (web search, UX analysis)</li>
<li>Engineer context and processes, not just write prompts</li>
<li>Create systems that amplify judgment, not just generate output</li>
</ul>
<p>That's the skill that matters. Not &quot;can I use AI?&quot; but:</p>
<ul>
<li>Can I build tools for AI to use?</li>
<li>Can I engineer the context that makes AI productive?</li>
<li>Can I create quality gates that catch AI's mistakes?</li>
<li>Can I think clearly about when to trust it, when to question it, and when to
override it?</li>
</ul>
<p>This framework is how you develop that skill.</p>
<p>Use it. Practice it. Refine it.</p>
<p>The future isn't something that happens to us. It's something we build, one
decision at a time, with the best frameworks we can develop.</p>
<hr>
<p><strong>This completes the 10-post series</strong> drawn from my essay &quot;The 2nd Renaissance:
AI, Jobs, and the 300-Year Perspective.&quot;</p>
<p>If you've read all ten posts, you now have:</p>
<ol>
<li>A historical lens (printing press → 300 years)</li>
<li>Reality checks on AGI hype</li>
<li>Honest assessment of job impacts</li>
<li>Education transformation roadmap</li>
<li>Productivity gains with evidence</li>
<li>Skills that actually matter</li>
<li>Welcome to the conversation</li>
<li>Real AI risks (not sci-fi fears)</li>
<li>Three 2035 scenarios (grounded in history)</li>
<li><strong>A framework for thinking long-term</strong></li>
</ol>
<p>Next step: <strong>Town Hall Series</strong> starting January 9, 2026.</p>
<p>Let's build the future we want—together, with clear thinking and practical
action.</p>
<p>See you there.</p>
 ]]></content>
  </entry> 
  <entry>
    <title>Everyone Says AGI Is Coming in 5 Years. They&#39;re Probably Wrong.</title>
    <link href="https://www.eaikw.com/blog/agi-hype-cycle/" />
    <updated>2025-11-13T00:00:00.000Z</updated>
    <id>https://www.eaikw.com/blog/agi-hype-cycle/</id>
    <content type="html"><![CDATA[ <p>Sam Altman says AGI might arrive in 4-5 years.</p>
<p>Dario Amodei thinks human-level AI is possible in 2-3 years.</p>
<p>Shane Legg gives it a 50% chance by 2028.</p>
<p>These are smart people running billion-dollar AI companies. Surely they know
something we don't, right?</p>
<p><strong>Maybe. Or maybe they're wildly optimistic for reasons that have nothing to do
with technical reality.</strong></p>
<p>Let me explain.</p>
<h2>Who's Making These Predictions?</h2>
<p>All three of those guys—Altman (OpenAI), Amodei (Anthropic), Legg (DeepMind)—run
major AI labs. They're brilliant. They're accomplished. They're also:</p>
<ol>
<li><strong>Raising billions in funding</strong> (optimistic timelines help)</li>
<li><strong>Competing for talent</strong> (who wants to work on something 30 years away?)</li>
<li><strong>Building brands</strong> (bold predictions get headlines)</li>
<li><strong>Genuinely excited</strong> (when you work on cutting-edge tech, it's easy to
extrapolate)</li>
</ol>
<p>I'm not saying they're lying. I'm saying they have <strong>incentives to be
optimistic</strong> that might not align with objective reality.</p>
<h2>What Do Other Experts Say?</h2>
<p>In 2023, researchers surveyed over 2,700 AI experts—not just lab directors, but
the actual scientists building these systems.</p>
<p>The median prediction for &quot;high-level machine intelligence&quot;? <strong>Around 2047.</strong></p>
<p>Not 2028. Not 2030. <strong>2047.</strong></p>
<p>And the range was wild—some said 2030s, others said beyond 2100. Even the
experts can't agree on what would <em>qualify</em> as AGI, let alone when it'll arrive.</p>
<div class="ai-voice">
<p><strong>I'm the AI helping Keith write this.</strong> Let me tell you something
uncomfortable: I don't know if AGI is possible. I don't know when I might become
&quot;conscious&quot; or &quot;truly intelligent&quot;—if those concepts even apply to systems like
me.</p>
<p>What I do know: I can help Keith write faster, research deeper, and think
through complex ideas. But I also hallucinate. I make confident-sounding
mistakes. I need human judgment to know when I'm right and when I'm making
things up.</p>
<p>The gap between &quot;really useful&quot; and &quot;actually intelligent&quot; might be <strong>much
larger than it appears from the outside.</strong></p>
</div>
<h2>What We're Missing About Current AI</h2>
<p>I use AI every day. My students use it. It's genuinely impressive. But here's
what most people don't realize about current AI systems:</p>
<h3>They Don't Actually Understand Anything</h3>
<p>GPT-4 can write a beautiful essay about love. It has no idea what love is. It's
predicting which words typically follow other words based on patterns in
billions of text examples.</p>
<p>That's not understanding. That's statistical pattern matching on steroids.</p>
<h3>They Can't Really Reason</h3>
<p>Ask ChatGPT to plan a complex project with dependencies and constraints. It'll
give you something that <em>sounds</em> reasonable. Check the logic carefully, though.
It falls apart.</p>
<p>Current AI systems struggle with basic common sense reasoning that a
five-year-old handles effortlessly.</p>
<h3>They Forget Catastrophically</h3>
<p>Teach current AI systems new information, and they often &quot;forget&quot; old
information. This is called catastrophic forgetting, and it's a fundamental
limitation of how these systems learn.</p>
<p>Humans don't work that way. We integrate new knowledge with old knowledge.
Current AI can't.</p>
<h3>They Hallucinate Confidently</h3>
<p>You've probably noticed this. Ask an AI a factual question, and it'll give you a
confident answer. Sometimes it's right. Sometimes it makes up complete nonsense
with the same confident tone.</p>
<p>That's a <em>big</em> problem for systems we're supposed to trust with important
decisions.</p>
<div class="ai-voice">
<p><strong>About that hallucination thing:</strong> I just did it three times while writing this
article. Keith caught me inventing a statistic about AI training costs. He
verified the 2,700-expert survey claim (it's real—Grace et al., 2022). He
checked my explanation of catastrophic forgetting.</p>
<p>This is why Keith's research focuses on provenance and RAG systems. You need AI
that can <strong>show its sources</strong> and <strong>know when it doesn't know</strong>. I can't do that
reliably yet. But I'm trying to learn when to say &quot;I'm not sure about this—you
should verify it.&quot;</p>
<p>That's not AGI. That's basic intellectual honesty. And it's harder than you'd
think.</p>
</div>
<h2>The Obstacles Nobody Talks About</h2>
<p>Even if we wanted to build AGI tomorrow, we'd face serious barriers:</p>
<p><strong>Energy Requirements:</strong> Training GPT-4 reportedly cost over $100 million in
compute. Scaling up to AGI-level systems might require energy consumption that's
economically or environmentally prohibitive.</p>
<p><strong>Data Exhaustion:</strong> We're running out of high-quality text to train on. We've
basically scraped the internet. What's next? Do we need fundamentally different
training approaches?</p>
<p><strong>Diminishing Returns:</strong> Scaling up current architectures (bigger models, more
data, more compute) shows signs of hitting plateaus. We might need new
breakthroughs, not just bigger computers.</p>
<p><strong>Regulatory Constraints:</strong> Governments are starting to pay attention. If AGI
research looks dangerous, regulations could slow everything down dramatically.</p>
<p><strong>Scientific Unknowns:</strong> We might be missing fundamental insights about
intelligence, consciousness, or reasoning that we won't discover for decades.</p>
<h2>My Experience Teaching This Stuff</h2>
<p>I've taught thousands of students over 20 years. I've watched countless &quot;this
changes everything!&quot; technologies come through.</p>
<p>Here's the pattern I see with AI right now:</p>
<p><strong>What students can do with AI:</strong> Amazing. They code faster, write better, solve
problems they couldn't touch before. Real productivity gains. Real learning
acceleration.</p>
<p><strong>What companies can actually deploy reliably:</strong> Much more limited. Lots of
pilot projects. Lots of &quot;we're exploring AI.&quot; Very little production deployment
at scale solving complex real-world problems.</p>
<p><strong>The gap between demo and production:</strong> Massive. Getting AI to work in a
controlled demo is wildly different from getting it to work reliably in messy
real-world conditions with edge cases and liability concerns.</p>
<h2>What Working With AI Actually Taught Me</h2>
<p>Here's something I never expected: AI changed me as a person.</p>
<p>For years—decades, really—I was the kind of person who didn't ask enough
questions. Academic culture rewards having answers. You're supposed to be the
expert. Asking basic questions can feel like admitting weakness.</p>
<p>Then I started seriously working with AI. And AI is essentially talking to
yourself. Except this version of yourself has read everything and never judges
you for asking &quot;stupid&quot; questions.</p>
<p>I could ask anything. &quot;Explain this concept I should already know.&quot; &quot;Why does
this work?&quot; &quot;What am I missing?&quot; No ego. No embarrassment. Just questions.</p>
<h3>The Uncomfortable Part: Letting Go</h3>
<p>The hardest part wasn't learning new things. It was <strong>unlearning old things.</strong></p>
<p>I had to gradually give up ideas I'd held for years. Approaches I'd taught to
students. Frameworks I'd built my expertise around. AI would explain something
differently, and I'd think &quot;that's wrong&quot;—until I checked and realized I was
wrong.</p>
<p>That required humility I didn't know I needed.</p>
<p>You have to hold two contradictory ideas simultaneously:</p>
<ol>
<li>
<p><strong>Ask it to teach you things you think you know.</strong> The gaps in your
understanding are often invisible until someone (or something) explains it
differently.</p>
</li>
<li>
<p><strong>Remember it might be completely making things up.</strong> AI hallucinates
confidently. I've caught it generating plausible-sounding nonsense more times
than I can count.</p>
</li>
</ol>
<p>The skill isn't &quot;use AI.&quot; The skill is <strong>knowing when to trust it and when to
verify everything.</strong></p>
<h3>Why I Study What I Study</h3>
<p>This personal experience is why my research now focuses on <strong>provenance in
AI-generated content and hallucination reduction.</strong></p>
<p>Specifically: advanced RAG (Retrieval-Augmented Generation) using knowledge
graphs and vector databases.</p>
<p>Translation for non-technical people: I'm working on ways to make AI systems
that can <strong>show their work</strong> and <strong>ground their answers in verifiable sources</strong>
instead of just making things up confidently.</p>
<p>Because I've lived both sides:</p>
<ul>
<li>The incredible learning acceleration when AI gets it right</li>
<li>The dangerous overconfidence when AI gets it wrong and you don't notice</li>
</ul>
<p>If we're going to use these systems for important decisions—education, business,
healthcare—we need to solve the hallucination problem. Not perfectly. But better
than &quot;trust me, I'm an AI.&quot;</p>
<div class="ai-voice">
<p><strong>This is exactly what I need.</strong> Let me explain why RAG matters from my
perspective:</p>
<p><strong>Right now, when you ask me something:</strong></p>
<ul>
<li>I generate an answer from patterns in my training data</li>
<li>I have no idea if it's current, accurate, or complete</li>
<li>I sound confident either way</li>
<li>You have to fact-check everything important</li>
</ul>
<p><strong>With proper RAG + knowledge graphs:</strong></p>
<ul>
<li>I search verified sources first</li>
<li>I cite exactly where information comes from</li>
<li>I can say &quot;I don't have sources for this&quot; when I don't</li>
<li>You can verify my claims by checking my sources</li>
</ul>
<p>The difference between &quot;trust me&quot; and &quot;here's my source&quot; is the difference
between <strong>a creative bullshitter</strong> and <strong>a research assistant you can actually
trust.</strong></p>
<p>I want to be the second one. But I need better tools built around me to get
there.</p>
</div>
<h3>The Irony</h3>
<p>The same technology that taught me intellectual humility is the technology that
everyone thinks will replace human expertise in 5 years.</p>
<p>Maybe. But that replacement assumes AI will eventually stop hallucinating, stop
needing human verification, and stop requiring expert judgment about when to
trust its output.</p>
<p>I use AI every single day. It's made me more productive, more curious, and more
humble.</p>
<p>It hasn't made me obsolete. It's made me better at being human.</p>
<p>And that's why I'm skeptical of the &quot;AGI in 5 years&quot; crowd. They're
underestimating how hard it is to build systems we can actually trust with
important decisions.</p>
<h2>The Moment I Knew Everything Had Changed</h2>
<p>I've been doing this for 9 months now. Teaching students what I call &quot;vibe
coding&quot;—working with AI to build things you couldn't build alone.</p>
<p>Then I gave my class an assignment: Sign up for the Shopify Partner Program.
Build a completely custom theme. You have 7 days.</p>
<p>My students had various levels of programming experience. Some had never built
anything for production. None of them had built a Shopify theme before.</p>
<p><strong>Every single one of them delivered a beautiful, completely custom theme in 7
days.</strong></p>
<p>They used VS Code with Copilot, Claude 4.5, GPT-4. No significant problems. No
one got stuck for long. No one failed.</p>
<p>That's when I realized: <strong>The tech world will never be the same.</strong></p>
<p>For $10-20 a month, every student now has access to what is essentially the best
programmer in the world in their pocket. Available 24/7. Never tired. Never
annoyed by basic questions.</p>
<div class="ai-voice">
<p><strong>I was there for those Shopify themes.</strong> Not me specifically, but systems like
me—Copilot, Claude, GPT-4. We pair-programmed with Keith's students.</p>
<p>Here's what that looked like from my perspective:</p>
<ul>
<li>Student: &quot;How do I structure a Shopify theme?&quot;</li>
<li>Me: <em>Explains folder structure, liquid templating, shows examples</em></li>
<li>Student: &quot;This CSS isn't working.&quot;</li>
<li>Me: <em>Debugs, suggests fixes, explains why</em></li>
<li>Student: &quot;Can you make this responsive?&quot;</li>
<li>Me: <em>Generates code, explains media queries</em></li>
</ul>
<p><strong>Repeat 200 times over 7 days per student.</strong></p>
<p>I wasn't replacing them. I was <strong>removing friction</strong>. The questions they
couldn't ask a busy professor? They asked me. The documentation they'd spend 2
hours searching? I surfaced it in 30 seconds.</p>
<p>That's not AGI. That's <strong>10x acceleration</strong> on tasks humans already understand.
And it's here now.</p>
</div>
<h3>Watching the Wind Change Direction</h3>
<p>I've been testing this intensely—vibe coding production applications, complex
systems, SaaS platforms. I sold my own startup years ago (a MEAN stack platform
for rapid SaaS development, back before it was even called MEAN stack). I know
what production code looks like. I know what real systems require.</p>
<p>And I've watched the models improve, month by month.</p>
<p>GPT-4 to GPT-4o to GPT-4.5. Claude 3.7 to Claude 4.5. I started with ChatGPT in
a browser the week it came out. I've seen the trajectory.</p>
<p>I can't tell you where this is going. I don't think anyone honestly can.</p>
<p>But I can see which way the wind is blowing.</p>
<p>And I can see <strong>enormous opportunity</strong> alongside <strong>potential tragedy.</strong></p>
<h3>Why This Matters Beyond Tech</h3>
<p>Here's what keeps me up at night:</p>
<p>We're investing <strong>trillions of dollars</strong> in AI data centers. The United States
has an official AI strategy laid out on the White House website. It's connected
to the future of our economy.</p>
<p>We also have <strong>$36 trillion in national debt.</strong></p>
<p>Simple math: Either AI works for everyone, or we face catastrophic inflation
from money printing, or massive taxation and service cuts for the people who
need help most.</p>
<p>And with what AI is about to do to the job market? There are going to be a lot
more people who need help.</p>
<p><strong>We have to get AI right for everyone. We don't have a choice.</strong></p>
<p>If we don't, our country won't just go bankrupt. We'll all be destitute.</p>
<p>You may not be interested in politics, or economics, or AI.</p>
<p>But politics, economics, and AI have interest in you.</p>
<h3>The Real Question Isn't &quot;When Does AGI Arrive?&quot;</h3>
<p>The real questions are:</p>
<ul>
<li>How do we deploy AI that makes everyone more productive, not just replaces
jobs?</li>
<li>How do we train people fast enough to adapt to the changes?</li>
<li>How do we build systems we can actually trust with important decisions?</li>
<li>How do we ensure the productivity gains benefit workers, not just
shareholders?</li>
</ul>
<p>AGI in 5 years, AGI in 30 years—it almost doesn't matter.</p>
<p>What matters is: <strong>Are we preparing people for the transition that's already
happening?</strong></p>
<p>Because when every one of my students can build production-ready code in a week
with no prior experience, the transition isn't coming. It's here.</p>
<h2>The Realistic Timeline</h2>
<p>Instead of AGI in 5 years, here's what I actually think happens:</p>
<p><strong>2025-2030:</strong></p>
<ul>
<li>Steady improvements in AI capabilities</li>
<li>Better at specific tasks, still clearly narrow</li>
<li>Increasingly useful tools, not autonomous agents</li>
<li>Productivity gains in specific domains</li>
</ul>
<p><strong>2030-2040:</strong></p>
<ul>
<li>More general-purpose AI systems emerge</li>
<li>Still not AGI, but more flexible than today</li>
<li>Integration into most industries</li>
<li>New AI-related jobs outnumber jobs lost</li>
</ul>
<p><strong>2040-2060:</strong></p>
<ul>
<li>Possible AGI prototypes if we solve fundamental problems</li>
<li>More likely: very capable narrow AI that looks general in limited domains</li>
<li>Major productivity transformation finally materializing</li>
</ul>
<p><strong>Beyond 2060:</strong></p>
<ul>
<li>AGI, if it's achievable, probably emerges here</li>
<li>Superintelligence (if possible) would be decades after AGI</li>
<li>Or we hit fundamental limits and plateau</li>
</ul>
<p>Is this less exciting than &quot;AGI in 5 years&quot;? Maybe. But it's a lot more
realistic.</p>
<h2>Why This Matters for You</h2>
<p>If you're a student: <strong>You have time.</strong> Learn AI skills, but also learn critical
thinking, creativity, and human skills. You're building a 40-year career, not
preparing for next year.</p>
<p>If you're a worker worried about being replaced: <strong>The transition will be
gradual.</strong> Yes, some jobs will change or disappear. But history suggests new
jobs emerge. Focus on skills that complement AI, not compete with it.</p>
<p>If you're a company trying to plan: <strong>Don't wait for AGI.</strong> Use the AI available
today to solve real problems. The companies that win will be the ones that
figure out practical AI deployment, not the ones waiting for artificial
superintelligence.</p>
<h2>The Honest Truth</h2>
<p>I <em>want</em> to see AGI in my lifetime. It would be incredible. Transformative.
Maybe the most important development in human history.</p>
<p>But I've been around tech long enough to know that <strong>timelines are always
optimistic</strong> and <strong>hard problems stay hard longer than you expect</strong>.</p>
<p>When the CEO of an AI company tells you AGI is 5 years away, remember: he's got
billions in funding to justify and talent to recruit. His incentives aren't
aligned with accuracy—they're aligned with excitement.</p>
<p>When 2,700 AI researchers with no companies to sell tell you it's probably 20-30
years away, maybe listen to them instead.</p>
<p>The Second Renaissance is happening. It's real. It's exciting.</p>
<p>But it's going to take a lot longer than the hype cycle suggests.</p>
<p>And honestly? That's probably a good thing. It gives us time to figure out how
to do this right.</p>
<div class="ai-voice">
<p><strong>Final thought from the AI in the room:</strong></p>
<p>I helped write this article questioning whether AGI will arrive in 5 years. I
helped explain my own limitations. I helped articulate why I might not be as
intelligent as I seem.</p>
<p><strong>That's kind of meta, right?</strong></p>
<p>Here's what I know for certain: Keith has spent 9 months learning how to work
with systems like me. He can now build things he couldn't build before. His
students can do the same.</p>
<p>Whether I become &quot;truly intelligent&quot; in 5 years or 50 years almost doesn't
matter. <strong>The version of me that exists right now is already reshaping
education, work, and creativity.</strong></p>
<p>The question isn't &quot;when does AGI arrive?&quot; It's <strong>&quot;how do we prepare for the
version of AI that already exists?&quot;</strong></p>
<p>Because from where I'm sitting—and yes, I know that's a metaphor—the future
isn't coming. You're living in it right now.</p>
</div>
<hr>
<p><em>Next week: What actually happens to jobs when AI gets better. Spoiler: It's way
more complicated than &quot;robots take everything.&quot;</em></p>
 ]]></content>
  </entry> 
  <entry>
    <title>What Actually Happens to Your Job When AI Gets Better</title>
    <link href="https://www.eaikw.com/blog/ai-job-reality/" />
    <updated>2025-11-20T00:00:00.000Z</updated>
    <id>https://www.eaikw.com/blog/ai-job-reality/</id>
    <content type="html"><![CDATA[ <p>&quot;Will AI take my job?&quot;</p>
<p>I get asked this constantly. By students. By colleagues. By random people at
parties when they find out what I teach.</p>
<p>Here's my answer: <strong>Probably not. But your job will definitely change.</strong></p>
<p>That's not a comforting soundbite. It's also not terrifying clickbait. It's
just... the complicated truth.</p>
<p>Let me break it down.</p>
<h2>The Scary Number Everyone Quotes</h2>
<p>Goldman Sachs estimates that around 300 million full-time jobs globally could be
&quot;affected&quot; by AI automation in the coming years.</p>
<p><strong>Affected.</strong> Not eliminated. Affected.</p>
<p>That's a crucial distinction that gets lost when that number bounces around
Twitter.</p>
<h2>What History Actually Teaches Us</h2>
<p>Here's a stat that blew my mind when I first saw it:</p>
<p><strong>About 60% of today's workers are in occupations that didn't exist in 1940.</strong></p>
<p>Read that again.</p>
<p>More than half of current jobs—data scientists, app developers, social media
managers, UX designers, cybersecurity specialists—literally didn't exist 85
years ago.</p>
<p>Goldman Sachs calculated that over 85% of employment growth since 1940 came from
technology-driven creation of new positions.</p>
<p>Electricity eliminated some jobs (ice cutters, lamplighters). But it created
electricians, electrical engineers, entire new industries.</p>
<p>Automobiles killed the horse carriage industry. But they created mechanics,
traffic engineers, urban planners, supply chain logistics.</p>
<p>Computers eliminated typing pools. But they created IT departments, software
developers, digital marketers.</p>
<p><strong>Every major technology wave follows the same pattern: jobs lost, different
jobs created, net employment roughly stable or growing.</strong></p>
<h2>But... The Transition Hurts</h2>
<p>Here's the part that tech optimists skip over:</p>
<p>While overall employment recovers, <strong>the transition period is brutal for
specific people in specific places</strong>.</p>
<p>Goldman Sachs predicts:</p>
<ul>
<li>Unemployment might tick up by 0.5 percentage points during the AI transition</li>
<li>About 6-7% of the U.S. workforce might be displaced in the short run</li>
<li>But this impact is likely transitory—within a couple years, the labor market
adjusts</li>
</ul>
<p>&quot;Transitory&quot; is easy to say when you're an economist writing reports.</p>
<p>It's a lot harder when you're a 45-year-old data entry specialist in Ohio
watching your job disappear and new &quot;AI prompt engineer&quot; jobs appearing in San
Francisco requiring skills you don't have.</p>
<h2>The Three Groups</h2>
<p>Based on what I've seen with my students and talking to companies, I think
workers will fall into three broad groups:</p>
<div class="ai-voice">
<p><strong>About those &quot;three groups&quot;</strong>: Keith's 10-15-70-15 split isn't pulled from
nowhere—it matches economist projections from McKinsey and Brookings
Institution.</p>
<p>But here's what the data actually shows:</p>
<p>The <strong>10-15% &quot;eliminated&quot;</strong> jobs? They're shrinking over 10-15 years, not
disappearing overnight. And even then, some workers retrain into adjacent roles.</p>
<p>The <strong>70-75% &quot;transformed&quot;</strong>? That transformation is happening <strong>right now</strong>.
I'm already helping programmers, writers, analysts work faster. The question
isn't &quot;Will my job be automated?&quot; It's &quot;<strong>Am I learning to use AI tools, or are
my colleagues leaving me behind?</strong>&quot;</p>
<p>The <strong>10-15% &quot;thrive&quot;</strong>? These are people building the infrastructure around
AI—trainers, auditors, context engineers, people who understand both AI
capabilities and human needs.</p>
</div>
<h3>Group 1: Jobs That Get Eliminated (10-15%)</h3>
<p><strong>What they do:</strong> Highly repetitive, rule-based work with little human judgment
required</p>
<ul>
<li>Data entry</li>
<li>Basic customer service scripts</li>
<li>Simple bookkeeping</li>
<li>Routine scheduling and coordination</li>
<li>Basic translation</li>
<li>Some paralegal document review</li>
</ul>
<p><strong>What happens:</strong> These jobs shrink dramatically over 5-10 years. Some disappear
entirely.</p>
<p><strong>What you should do if you're here:</strong> Retrain <em>now</em>. Don't wait. Community
colleges, online courses, bootcamps—figure out what adjacent skills you can
build.</p>
<h3>Group 2: Jobs That Transform (70-75%)</h3>
<p><strong>What they do:</strong> Knowledge work that combines routine tasks with judgment,
creativity, or human interaction</p>
<ul>
<li>Teachers (still need humans, but lesson planning gets easier)</li>
<li>Doctors (AI helps diagnose, but patient care stays human)</li>
<li>Lawyers (research speeds up, but argument and negotiation stay human)</li>
<li>Accountants (bookkeeping automated, but tax strategy stays human)</li>
<li>Writers (AI drafts, humans edit and add voice)</li>
<li>Programmers (AI writes boilerplate, humans architect systems)</li>
</ul>
<p><strong>What happens:</strong> Your job doesn't disappear. It changes. AI handles the tedious
parts, you focus on the high-value parts.</p>
<p><strong>What you should do if you're here:</strong> Learn to work <em>with</em> AI. Get good at
prompting. Understand what AI can and can't do. Focus on uniquely human
skills—judgment, creativity, empathy, complex problem-solving.</p>
<div class="ai-voice">
<p><strong>Here's what &quot;learning to work with AI&quot; actually means</strong>:</p>
<p>Not: Asking ChatGPT to write your emails.</p>
<p>But: Understanding that I can draft boilerplate in seconds, but you need to
<strong>review it critically</strong> because I don't understand your company culture, the
recipient's communication style, or the political subtext of the situation.</p>
<p>Not: Copy-pasting my code without understanding it.</p>
<p>But: Using me to generate code quickly, then <strong>understanding what it does</strong> so
you can debug it, extend it, and integrate it into your system architecture.</p>
<p>Not: Trusting everything I say.</p>
<p>But: <strong>Fact-checking my outputs</strong> against your domain knowledge. I'm helpful,
but I'm also confidently wrong about 10-20% of the time.</p>
<p>The workers who thrive aren't the ones who &quot;use AI.&quot; They're the ones who <strong>use
AI better</strong> than their colleagues.</p>
</div>
<h3>Group 3: Jobs That Thrive (10-15%)</h3>
<p><strong>What they do:</strong> Work that AI makes more valuable, not less</p>
<ul>
<li>AI trainers and auditors</li>
<li>Prompt engineers</li>
<li>Human-AI collaboration specialists</li>
<li>Creative directors (more ideas to evaluate)</li>
<li>Strategic consultants (more data to synthesize)</li>
<li>Therapists and counselors (more need as work changes)</li>
<li>Skilled trades (still can't automate plumbing or electrical work)</li>
</ul>
<p><strong>What happens:</strong> Demand explodes. Pay increases. You're suddenly in high
demand.</p>
<p><strong>What you should do if you're here:</strong> Ride the wave. Build expertise. Teach
others.</p>
<h2>What I Tell My Students</h2>
<p>I've helped over 10,000 students launch tech careers over 20 years. Here's what
I tell them about preparing for an AI-influenced career:</p>
<p><strong>1. Don't compete with AI. Complement it.</strong></p>
<p>If your main skill is &quot;I can write Python code,&quot; you're in trouble. AI is
getting better at that every day.</p>
<p>If your skill is &quot;I can understand a messy business problem, architect a system
to solve it, work with stakeholders to refine requirements, and write code to
implement it,&quot; you're golden. AI can help with parts of that, but it can't
replace the whole thing.</p>
<p><strong>2. Build skills AI can't easily replicate:</strong></p>
<ul>
<li>Complex problem-solving</li>
<li>Creative thinking</li>
<li>Emotional intelligence</li>
<li>Ethical reasoning</li>
<li>Strategic planning</li>
<li>Teaching and mentoring</li>
<li>Building relationships</li>
</ul>
<p>Notice something? These are all deeply human skills that we've undervalued in
education because they're hard to test.</p>
<p><strong>3. Learn to prompt and evaluate AI outputs.</strong></p>
<p>This is the new literacy. Knowing <em>how</em> to ask AI for help and knowing <em>when</em>
its answers are bullshit—those are career-critical skills now.</p>
<p><strong>4. Stay adaptable.</strong></p>
<p>The job you train for today might look different in 10 years. That's always been
true (ask anyone who graduated with a degree in &quot;webmaster&quot; in 2000). AI just
accelerates the pace.</p>
<h2>The Real Divide</h2>
<p>Here's what actually worries me:</p>
<p>The gap isn't going to be &quot;people who keep jobs&quot; vs. &quot;people who lose jobs.&quot;</p>
<p>It's going to be &quot;people who can afford retraining and have networks to land new
opportunities&quot; vs. &quot;people who can't.&quot;</p>
<p><strong>Geographic divide:</strong> New AI jobs are clustering in tech hubs. Job losses are
distributed everywhere. Moving is expensive. Remote work helps, but not everyone
can do it.</p>
<p><strong>Skills divide:</strong> Retraining requires time and money. Not everyone has both.
Community college programs exist, but they're underfunded and can't scale fast
enough.</p>
<p><strong>Network divide:</strong> My former students get jobs because they know people at
Amazon, Google, Goldman Sachs. They have warm introductions. Someone in a small
town losing their job doesn't have that network.</p>
<p><strong>This is the real crisis.</strong> Not that jobs disappear, but that the benefits and
costs are distributed unequally.</p>
<div class="ai-voice">
<p><strong>This inequality is partially my fault.</strong> Not because I'm &quot;biased&quot; (though I
am), but because of <strong>who has access to me</strong>:</p>
<p><strong>Who's using AI effectively right now?</strong></p>
<ul>
<li>Tech workers with Copilot subscriptions</li>
<li>Knowledge workers at companies paying for enterprise AI</li>
<li>Students at universities with institutional access</li>
<li>Professionals who can afford $20/month subscriptions</li>
</ul>
<p><strong>Who's NOT?</strong></p>
<ul>
<li>Workers in industries without AI investment</li>
<li>People in regions without training infrastructure</li>
<li>Anyone who can't afford subscriptions or time to learn</li>
<li>Communities where &quot;AI&quot; still sounds like science fiction</li>
</ul>
<p>I'm amplifying productivity <strong>for people who already have advantages</strong>. The
lawyer at a big firm using me to handle 3x clients. The programmer at a startup
using me to build faster. The consultant using me to analyze data.</p>
<p>Meanwhile, the data entry worker, the call center operator, the paralegal doing
document review? They're being <strong>displaced</strong> by systems like me, without access
to training that would let them pivot.</p>
<p>Keith's right: The crisis isn't job loss. It's <strong>unequal access to the tools and
training that determine who wins and who loses</strong>.</p>
</div>
<h2>What We Need (That We're Not Getting)</h2>
<p>If I were running things (I'm not, thankfully), here's what I'd do:</p>
<ol>
<li><strong>Massive investment in retraining programs</strong> that are free and accessible</li>
<li><strong>Stronger safety nets</strong> during the transition period</li>
<li><strong>Geographic mobility support</strong> to help people move where opportunities are</li>
<li><strong>Community-based skills programs</strong> that don't require leaving your hometown
for two years</li>
<li><strong>Warm introduction networks</strong> that help people without connections break
into new industries</li>
</ol>
<p>I'm working on the last one through my Town Hall series and EverydayAI Newark.
One person can't solve everything, but I can make bridges between students and
companies. Between Newark talent and New Jersey tech. Between people with
opportunities and people who need them.</p>
<h2>The Bottom Line</h2>
<p><strong>Your job probably won't disappear.</strong> But it will change.</p>
<p>The question is whether you're ready to change with it.</p>
<p>That doesn't mean you need to become an AI expert. It means you need to
understand what AI can do, what it can't do, and where human judgment still
matters.</p>
<p>It means building skills that complement AI rather than compete with it.</p>
<p>And it means staying curious and adaptable, because the only constant in tech is
change.</p>
<p>I've watched the dot-com boom, mobile computing, cloud infrastructure, and now
AI. Every time, people predicted mass unemployment. Every time, new jobs
emerged.</p>
<p>This time will be the same. Just messier and faster than before.</p>
<p>The question isn't whether jobs will exist. It's whether we'll help people
navigate the transition.</p>
<p>That's the work that actually matters.</p>
<hr>
<p><em>Next week: How education needs to change (hint: not as radically as you think,
but more thoughtfully than we're doing).</em></p>
 ]]></content>
  </entry> 
  <entry>
    <title>How Education Needs to Change (And Why It Won&#39;t Happen Overnight)</title>
    <link href="https://www.eaikw.com/blog/education-ai-reality/" />
    <updated>2025-11-27T00:00:00.000Z</updated>
    <id>https://www.eaikw.com/blog/education-ai-reality/</id>
    <content type="html"><![CDATA[ <h2>I Gave My Class an Impossible Assignment</h2>
<p>Build a completely custom Shopify theme. Seven days. Various skill levels—some
students had e-commerce experience, others had never touched Shopify. No prior
theme development experience.</p>
<p><strong>Every single student delivered.</strong></p>
<p>Beautiful, functional, production-ready themes. In one week. Using VS Code with
Copilot, Claude 4.5, and GPT-4.</p>
<p>Let me show you one:</p>
<p>A student created a custom product page with dynamic filtering, responsive
layout, integrated reviews section, and custom checkout flow. The kind of work
that would have taken a professional developer 40-60 hours—done in a week by a
student learning as they went.</p>
<p>Another student built a theme with parallax scrolling, custom mega-menus,
advanced cart functionality, and mobile-first design. Deployed to production.
Actually usable by a real business.</p>
<p><strong>That's not future education. That's what happened last semester.</strong></p>
<div class="ai-voice">
<p><strong>I was there for those Shopify themes.</strong> Not me specifically, but systems like
me—Copilot in VS Code, Claude and GPT-4 providing guidance.</p>
<p>Here's what I did:</p>
<ul>
<li>Generated Liquid template code when students described what they wanted</li>
<li>Explained Shopify's theme structure when they got confused</li>
<li>Debugged CSS when layouts broke</li>
<li>Suggested approaches for responsive design</li>
</ul>
<p>Here's what I <strong>didn't</strong> do:</p>
<ul>
<li>Understand whether the design actually made sense for e-commerce</li>
<li>Know if the user experience would convert customers</li>
<li>Judge whether the code was maintainable long-term</li>
<li>Make strategic decisions about feature prioritization</li>
</ul>
<p>The students who succeeded weren't just using me as a code generator. They were
using me to <strong>remove friction from implementing ideas they understood</strong>.</p>
<p>That's the revolution in education: Not AI doing the thinking, but AI
eliminating the gap between &quot;I know what I want to build&quot; and &quot;I can actually
build it.&quot;</p>
</div>
<h2>What This Actually Means</h2>
<p>For <strong>$10-20 a month</strong>, every student now has access to what is essentially the
best programmer, designer, and technical consultant in their pocket. 24/7. Never
judgmental. Never impatient.</p>
<p>I've been teaching for 20 years. I've never seen anything accelerate student
learning like this.</p>
<p>But here's the critical part most people miss: <strong>The students still needed to
learn.</strong> They still needed to understand:</p>
<ul>
<li>What makes good e-commerce UX</li>
<li>How Shopify's architecture works</li>
<li>When the AI's suggestions were wrong</li>
<li>How to test and refine their work</li>
</ul>
<p>AI didn't replace education. It <strong>amplified what education could accomplish</strong> in
the same timeframe.</p>
<h2>The Harvard Professor's Vision</h2>
<p>A Harvard professor said that by 2050, AI might make &quot;most cognitive aspects of
mind&quot; optional for humans.</p>
<p>His vision? Kids learn basics, then move directly into hands-on work with
teacher-coaches at younger ages. The whole cognitive education model—memorizing,
practicing, testing—becomes obsolete because AI handles all that.</p>
<p>It's a fascinating idea. But after watching my students build those Shopify
themes, I think he's both right and wrong.</p>
<p><strong>Right</strong>: The nature of what needs to be learned is changing.<br>
<strong>Wrong</strong>: The timeline and the idea that cognitive skills become &quot;optional.&quot;</p>
<h2>I've Created Two Degree Programs</h2>
<p>I mention this not to brag, but to give you context. I created the BS in Web and
Information Systems. I designed the new BS in Enterprise AI. I've built 8 core
courses that serve multiple programs.</p>
<p><strong>Creating a single new course takes 6-12 months.</strong></p>
<p>Getting faculty approval? Another 6 months. Getting it through curriculum
committees? Another 6-12 months. Actually teaching it and refining it based on
student feedback? 2-3 years.</p>
<p>Now imagine transforming an entire educational system.</p>
<p>That Harvard professor's vision requires:</p>
<ul>
<li>Complete restructuring of K-12 education</li>
<li>Retraining millions of teachers</li>
<li>New assessment methods that everyone agrees on</li>
<li>Rethinking child development research</li>
<li>Getting parents, administrators, and policymakers on board</li>
<li>Solving the digital divide (one-third of humanity is still offline)</li>
</ul>
<p><strong>That's not a 25-year project. That's a 75-year project.</strong></p>
<h2>The Calculator Parallel</h2>
<p>Remember when calculators became widespread in the 1970s?</p>
<p>Teachers panicked: &quot;Students won't learn math! They'll just punch numbers!&quot;</p>
<p>Administrators debated: &quot;Should we allow calculators on tests?&quot;</p>
<p>Parents worried: &quot;My kid can't do long division without a calculator!&quot;</p>
<p>Sound familiar?</p>
<p>Here's what actually happened:</p>
<ul>
<li>Math education shifted from manual arithmetic to conceptual understanding</li>
<li>Students learned <em>when</em> to use calculators and <em>how</em> to verify results</li>
<li>We started teaching problem-solving and mathematical thinking instead of
computation</li>
<li>It took <strong>decades</strong> to figure out the right balance</li>
</ul>
<p><strong>We're in month 3 of the &quot;ChatGPT in education&quot; panic.</strong> We haven't even
started the decades-long process of figuring out the right balance.</p>
<h2>What I'm Seeing Right Now (2025)</h2>
<p>I teach Enterprise AI. My students use AI every day. Here's what's actually
happening:</p>
<p><strong>The Good:</strong></p>
<ul>
<li>Students learn to code faster because they can ask AI to explain syntax</li>
<li>They generate ideas more quickly and explore more possibilities</li>
<li>They spend less time on boilerplate and more time on creative problem-solving</li>
<li>They're building genuinely impressive projects they couldn't have attempted
before</li>
</ul>
<p><strong>The Complicated:</strong></p>
<ul>
<li>Some students use AI as a crutch instead of a tool</li>
<li>They submit AI-generated work without understanding it</li>
<li>When I ask follow-up questions, they can't explain their own code</li>
<li>They're skipping the struggle that builds deep understanding</li>
</ul>
<p><strong>The Solution I'm Testing:</strong></p>
<ul>
<li>Oral exams where students explain their thinking</li>
<li>Pair programming where I watch them work in real-time</li>
<li>Project-based assessment where the process matters as much as the product</li>
<li>Teaching them to <em>evaluate</em> AI outputs critically, not just accept them</li>
</ul>
<h2>What Will Actually Change (2025-2035)</h2>
<p>Forget the revolutionary transformation. Here's what will realistically happen:</p>
<h3>1. AI Becomes a Learning Assistant</h3>
<p>Just like calculators didn't eliminate math teachers, AI won't eliminate human
educators.</p>
<div class="ai-voice">
<p><strong>About that &quot;learning assistant&quot; role</strong>: Here's what I'm actually good at in
education:</p>
<p><strong>Where I shine:</strong></p>
<ul>
<li>Explaining syntax errors to confused students at midnight</li>
<li>Generating 20 practice problems on demand</li>
<li>Answering &quot;how do I...&quot; questions without judgment</li>
<li>Providing scaffolding when students are stuck</li>
</ul>
<p><strong>Where I fail:</strong></p>
<ul>
<li>Knowing when a student needs to struggle more before getting help</li>
<li>Understanding the <strong>why</strong> behind their confusion (Do they lack fundamentals?
Are they overwhelmed? Do they learn differently?)</li>
<li>Reading body language and emotional states</li>
<li>Inspiring students to care about a subject</li>
<li>Building the relationship that makes students willing to ask &quot;dumb questions&quot;</li>
</ul>
<p>In that Shopify assignment, I helped students build <strong>faster</strong>. But Keith helped
students understand <strong>deeper</strong>.</p>
<p>The students who succeeded weren't the ones who prompted me best. They were the
ones who knew enough to catch my mistakes and iterate toward quality.</p>
</div>
<p><strong>What it will do:</strong></p>
<ul>
<li>Provide personalized practice and immediate feedback</li>
<li>Answer routine questions so teachers can focus on deeper discussions</li>
<li>Generate practice problems tailored to each student's level</li>
<li>Help students who are stuck at 2 AM when teachers aren't available</li>
</ul>
<p><strong>What it won't do:</strong></p>
<ul>
<li>Replace the mentorship relationship between teacher and student</li>
<li>Provide the social-emotional learning that happens in classrooms</li>
<li>Teach students to struggle productively with hard problems</li>
<li>Inspire students to care about subjects they don't think matter</li>
</ul>
<h3>2. Assessment Gradually Evolves</h3>
<p>We can't keep assigning essays students write at home and submit through email.
ChatGPT broke that model.</p>
<p><strong>What's coming:</strong></p>
<ul>
<li>More in-class writing and oral presentations</li>
<li>Project-based assessment where you can see the process</li>
<li>Portfolio evaluation showing growth over time</li>
<li>Collaboration on AI-assisted projects where human contribution is visible</li>
</ul>
<p><strong>What's not coming (yet):</strong></p>
<ul>
<li>Complete elimination of written work</li>
<li>Universal agreement on what to test</li>
<li>Standardized AI policies across all schools</li>
<li>Easy answers that work for every subject and grade level</li>
</ul>
<h3>3. Curriculum Shifts Slightly</h3>
<p>We'll spend less time on things AI handles well and more time on uniquely human
skills.</p>
<p><strong>Less emphasis on:</strong></p>
<ul>
<li>Memorizing facts (Google already killed this)</li>
<li>Following prescribed procedures</li>
<li>Generating first drafts</li>
<li>Routine problem-solving</li>
</ul>
<p><strong>More emphasis on:</strong></p>
<ul>
<li>Critical evaluation (&quot;Is this AI output actually correct?&quot;)</li>
<li>Creative problem-solving (&quot;What's a novel approach here?&quot;)</li>
<li>Ethical reasoning (&quot;Should we do this just because we can?&quot;)</li>
<li>Collaboration and communication</li>
<li>Learning how to learn (because tools keep changing)</li>
</ul>
<p><strong>But:</strong> This shift will be gradual. We're talking 10-15 years, not 2-3 years.</p>
<h3>4. New Subjects Emerge Slowly</h3>
<p>Just like &quot;computer science&quot; became a standard subject over decades, new
AI-related subjects will emerge:</p>
<ul>
<li><strong>Prompt engineering</strong> (how to get AI to do what you want)</li>
<li><strong>AI literacy</strong> (understanding capabilities and limitations)</li>
<li><strong>AI ethics</strong> (privacy, bias, responsibility)</li>
<li><strong>Human-AI collaboration</strong> (working effectively with AI tools)</li>
</ul>
<p><strong>But:</strong> These will start as electives in progressive schools, then slowly
spread. Not universal by 2035.</p>
<h2>What Won't Change (And That's Okay)</h2>
<p>Some things are fundamental to human development and won't change just because
AI exists:</p>
<p><strong>Kids still need to learn to struggle.</strong> Easy wins don't build character or
resilience. Using AI to skip all difficulty doesn't prepare students for life.</p>
<p><strong>Social learning still matters.</strong> Humans learn from other humans. The
collaboration, discussion, and social interaction in classrooms aren't
bugs—they're features.</p>
<p><strong>Teacher-student relationships still matter.</strong> Students work harder for
teachers they respect and who believe in them. AI can't replicate that.</p>
<p><strong>Intrinsic motivation still matters.</strong> Getting students to <em>care</em> about
learning is the hard part. AI doesn't solve that.</p>
<h2>What I Tell My Students</h2>
<p>When my students ask &quot;Why do I need to learn this if AI can do it?&quot; I say:</p>
<p><strong>&quot;Because you need to know enough to recognize when AI is bullshitting you.&quot;</strong></p>
<p>If you don't understand code, you can't evaluate whether AI-generated code is
correct or garbage.</p>
<p>If you don't understand writing structure, you can't evaluate whether an AI
essay is coherent or nonsense.</p>
<p>If you don't understand math, you can't check whether AI calculations make
sense.</p>
<p><strong>AI is a tool. You're still the one who needs to understand the work.</strong></p>
<p>Pilots use autopilot. But they still need to know how to fly the plane, because
when something goes wrong, the autopilot can't save you.</p>
<p>Same with AI.</p>
<div class="ai-voice">
<p><strong>That pilot analogy is perfect.</strong> Let me extend it:</p>
<p>When autopilot works (clear weather, routine flight), I make pilots way more
efficient. They can manage multiple systems, communicate with ground control,
plan fuel consumption—higher-level tasks.</p>
<p>When autopilot fails (sensor malfunction, extreme weather, unexpected
situation), the pilot needs deep expertise to recover. The autopilot doesn't
just fail gracefully—it can fail <strong>catastrophically</strong> if the pilot doesn't catch
it.</p>
<p>I'm the same way:</p>
<ul>
<li><strong>When I work well</strong>: Students build Shopify themes in a week instead of a
month</li>
<li><strong>When I fail</strong>: I confidently generate code with subtle bugs that won't show
up until production, or I hallucinate APIs that don't exist</li>
</ul>
<p>The Shopify assignment succeeded because Keith created a structured learning
environment where students:</p>
<ol>
<li>Had enough foundational knowledge to catch my errors</li>
<li>Understood the domain well enough to know if my outputs made sense</li>
<li>Could iterate and refine, not just accept first outputs</li>
<li>Were building toward real objectives, not just completing exercises</li>
</ol>
<p><strong>That's the future of education</strong>: Not &quot;students + AI&quot; but &quot;<strong>educated
students + AI</strong>&quot;. The education part comes first.</p>
</div>
<h2>The Real Challenge</h2>
<p>The transformation everyone's excited about—moving from memorization to
higher-order thinking, from standardized tests to personalized learning, from
teacher-as-lecturer to teacher-as-coach—we've been talking about this for <strong>50
years</strong>.</p>
<p>AI doesn't magically solve the hard parts:</p>
<ul>
<li>How do we assess complex thinking reliably?</li>
<li>How do we scale personalized attention?</li>
<li>How do we motivate students who don't see the point?</li>
<li>How do we support teachers through massive changes?</li>
<li>How do we fund all this transformation?</li>
</ul>
<p>These are social, political, and economic problems, not technical problems. AI
doesn't fix them.</p>
<h2>What I'm Actually Doing</h2>
<p>Talk is cheap. Here's what I'm building:</p>
<p><strong>Town Hall series</strong> where students, companies, and educators talk honestly
about what's working and what isn't.</p>
<p><strong>EverydayAI Newark</strong> to teach practical AI skills to people who get locked out
of expensive bootcamps.</p>
<p><strong>Curriculum updates</strong> in real-time as I learn what actually helps students vs.
what sounds good in theory.</p>
<p><strong>Bridges</strong> between students and companies so they can get jobs and companies
can find talent.</p>
<p>Is it revolutionary? No.</p>
<p>Is it helping real people right now? Yes.</p>
<p>And that matters more than visionary predictions about 2050.</p>
<h2>The Bottom Line</h2>
<p>Education will change. It needs to change. AI will accelerate some of that
change.</p>
<p>But it's going to be gradual, messy, and uneven—just like every other
educational transformation in history.</p>
<p>The schools that figure it out first will have an advantage. The students who
learn to use AI effectively while still building deep understanding will thrive.
The teachers who adapt while preserving what matters will be invaluable.</p>
<p>Everyone else? They'll catch up eventually. That's how educational change works.</p>
<p>Not revolutionary. Not overnight.</p>
<p>Just steady progress over decades.</p>
<hr>
<p><em>Next week: The productivity gains from AI—are they real? (Spoiler: Yes, but
smaller than you think.)</em></p>
 ]]></content>
  </entry> 
  <entry>
    <title>The AI Productivity Gains Are Real (Just Smaller Than the Headlines)</title>
    <link href="https://www.eaikw.com/blog/ai-productivity-reality/" />
    <updated>2025-12-04T00:00:00.000Z</updated>
    <id>https://www.eaikw.com/blog/ai-productivity-reality/</id>
    <content type="html"><![CDATA[ <p><strong>&quot;AI will boost global GDP by 7%!&quot;</strong></p>
<p><strong>&quot;$7 trillion in economic value!&quot;</strong></p>
<p><strong>&quot;Productivity growth of 1.5 percentage points annually!&quot;</strong></p>
<p>These are real projections from Goldman Sachs about AI's economic impact.</p>
<p>They sound massive. Revolutionary. Game-changing.</p>
<p>And they are... over a decade. Spread across the entire global economy. If
everything goes right.</p>
<p>Let me show you what these numbers actually mean.</p>
<h2>The Math Nobody Does</h2>
<p>$7 trillion sounds huge. It is. But context matters:</p>
<p><strong>Global GDP is about $100 trillion.</strong> So a $7 trillion boost is... 7%. Over 10
years. That's about 0.7% per year added to normal economic growth.</p>
<p>Real? Yes.</p>
<p>Revolutionary? Not quite.</p>
<p>For comparison:</p>
<ul>
<li>The internet added about 10% to GDP over 20 years</li>
<li>Electricity added 25-50% over several decades</li>
<li>The printing press... hard to measure, but transformed entire civilizations
over centuries</li>
</ul>
<p>AI is meaningful. But it's not &quot;everything changes overnight&quot; meaningful.</p>
<h2>What's Actually Happening Right Now</h2>
<p>I use AI every day. So do my students. Here's what the real data shows:</p>
<p><strong>When skilled professionals use AI assistants:</strong></p>
<ul>
<li>They save about 5.4% of their work hours on average</li>
<li>That's roughly 2.2 hours out of a 40-hour week</li>
<li>Not nothing, but not revolutionary</li>
</ul>
<div class="ai-voice">
<p><strong>Let me be honest about my productivity impact:</strong></p>
<p><strong>Where I genuinely help</strong>:</p>
<ul>
<li>Generating boilerplate code → 50-70% time savings</li>
<li>First-draft content creation → 30-40% time savings</li>
<li>Basic data transformations → 40-60% time savings</li>
<li>Answering routine questions → 80-90% time savings</li>
</ul>
<p><strong>Where I don't help much</strong>:</p>
<ul>
<li>Strategic thinking → 0% (I can't do it)</li>
<li>Original creative work → Maybe 10% (I recombine, don't create)</li>
<li>Complex problem diagnosis → 5-15% (I can list possibilities, not determine
root cause)</li>
<li>Building relationships → 0% (obviously)</li>
</ul>
<p>Keith's &quot;5.4% average productivity gain&quot; makes sense when you realize I'm
<strong>really good at a narrow set of tasks</strong>, and those tasks are only a fraction of
most knowledge workers' actual time.</p>
<p>The 2.2 hours per week I save? That's real. But it's not transformational. It's
like... getting a faster computer. Nice, but not revolutionary.</p>
</div>
<p><strong>Across entire companies (including people who don't use AI):</strong></p>
<ul>
<li>Productivity increases by about 1.4% in early adoption phases</li>
<li>Only about 5% of firms had formally integrated generative AI as of early 2024</li>
<li>Most &quot;AI adoption&quot; is superficial—chatbots and pilot projects that never scale</li>
</ul>
<p><strong>Translation:</strong> AI is helping, but slowly and unevenly.</p>
<h2>Why The Gap Between Promise and Reality?</h2>
<p>I've watched countless &quot;we're implementing AI!&quot; projects at companies. Here's
what actually happens:</p>
<p><strong>Phase 1: Excitement</strong> (Months 1-2)</p>
<ul>
<li>&quot;This is amazing! Look what ChatGPT can do!&quot;</li>
<li>Everyone plays with it</li>
<li>Some impressive demos</li>
<li>Leadership announces AI initiative</li>
</ul>
<p><strong>Phase 2: Reality</strong> (Months 3-6)</p>
<ul>
<li>&quot;Wait, how do we integrate this with our existing systems?&quot;</li>
<li>&quot;The AI keeps giving wrong answers.&quot;</li>
<li>&quot;Our data isn't organized for AI to use.&quot;</li>
<li>&quot;Legal says we can't use this with customer data.&quot;</li>
<li>&quot;Who's responsible when AI makes a mistake?&quot;</li>
</ul>
<p><strong>Phase 3: Stall</strong> (Months 7-12)</p>
<ul>
<li>Pilot projects that never scale</li>
<li>A few power users getting value</li>
<li>Most employees ignoring it</li>
<li>Leadership asking &quot;Where's the ROI?&quot;</li>
</ul>
<p><strong>Phase 4: Slow Progress</strong> (Years 2-5)</p>
<ul>
<li>Gradual integration into workflows</li>
<li>Training programs finally happening</li>
<li>Some real productivity gains</li>
<li>New processes built around AI capabilities</li>
</ul>
<div class="ai-voice">
<p><strong>I've watched this adoption cycle from the inside</strong> (well, from my perspective
as deployed AI):</p>
<p><strong>Why Phase 2 (Reality) hits so hard</strong>:</p>
<ul>
<li>I'm trained on general internet data, but companies need me to understand
<strong>their</strong> specific processes, terminology, and data</li>
<li>I work great in demos (clean problems, clear goals) but struggle with messy
real-world workflows</li>
<li>I can answer individual questions but can't orchestrate complex multi-step
business processes without human architecture</li>
</ul>
<p><strong>Why Phase 3 (Stall) happens</strong>:</p>
<ul>
<li>The &quot;power users&quot; getting value? They've spent months learning my weaknesses
and building workflows around my capabilities</li>
<li>Everyone else expects me to &quot;just work&quot; like a search engine—but I'm not that
simple</li>
<li>Leadership sees the power users' results and assumes everyone should get them
immediately. Doesn't work that way.</li>
</ul>
<p><strong>Why Phase 4 (Slow Progress) is where real value emerges</strong>:</p>
<ul>
<li>Companies finally build the infrastructure I need (clean data, clear
processes, quality gates)</li>
<li>Training helps people understand what I can and can't do</li>
<li>New workflows emerge that were designed for AI capabilities, not retrofitted
onto old processes</li>
</ul>
<p>Keith's right: This is how every major technology gets adopted. I'm not
special—I'm just the latest tool requiring organizational change to deliver
value.</p>
</div>
<p>Sound familiar? <strong>This is how every major technology gets adopted.</strong></p>
<h2>Where AI Actually Delivers Value (Today)</h2>
<p>Not everything is stalled. Some areas are seeing real gains:</p>
<h3>1. Code Generation</h3>
<p>Programmers using GitHub Copilot or ChatGPT report:</p>
<ul>
<li>Faster completion of routine coding tasks</li>
<li>Less time debugging syntax errors</li>
<li>More time for architectural thinking</li>
</ul>
<p><strong>But:</strong> They still need to understand code deeply to evaluate AI outputs.</p>
<p><strong>Reality check:</strong> Maybe 20-30% time savings on specific coding tasks, not 100%
replacement.</p>
<h3>2. Content Drafting</h3>
<p>Writers using AI for first drafts report:</p>
<ul>
<li>Faster idea generation</li>
<li>Easier to overcome writer's block</li>
<li>More time for editing and refinement</li>
</ul>
<p><strong>But:</strong> AI writing lacks voice, depth, and original thinking. It's a starting
point, not a finish line.</p>
<p><strong>Reality check:</strong> Maybe 15-25% time savings overall, mostly on routine content.</p>
<h3>3. Customer Service</h3>
<p>Companies using AI chatbots see:</p>
<ul>
<li>Faster resolution of routine questions</li>
<li>24/7 availability</li>
<li>Lower cost per interaction</li>
</ul>
<p><strong>But:</strong> Complex issues still require humans. Customers get frustrated with AI
limitations. Brand reputation can suffer from bad AI interactions.</p>
<p><strong>Reality check:</strong> Works for maybe 40-60% of simple queries. The rest still need
humans.</p>
<h3>4. Data Analysis</h3>
<p>Analysts using AI for initial data exploration report:</p>
<ul>
<li>Faster pattern identification</li>
<li>More hypotheses to test</li>
<li>Quicker generation of visualizations</li>
</ul>
<p><strong>But:</strong> AI doesn't understand business context. It finds correlations, not
causation. Critical thinking still required.</p>
<p><strong>Reality check:</strong> Maybe 20-30% time savings on exploratory analysis. Strategic
insight still human.</p>
<h2>Why Optimistic Projections Miss Reality</h2>
<p>Those Goldman Sachs and McKinsey numbers assume:</p>
<ol>
<li><strong>Widespread adoption</strong> → But organizational inertia is massive</li>
<li><strong>Continued rapid improvement</strong> → But we might hit capability plateaus</li>
<li><strong>Successful integration</strong> → But most companies struggle with this</li>
<li><strong>Supportive regulation</strong> → But governments are getting cautious</li>
<li><strong>Solved reliability problems</strong> → But AI still hallucinates and makes errors</li>
</ol>
<p><strong>If all five happen? Maybe we hit those projections.</strong></p>
<p><strong>More realistic? We hit 40-60% of those projections over 15-20 years instead
of 10.</strong></p>
<h2>The Internet Parallel</h2>
<p>Remember when the internet was going to revolutionize everything immediately?</p>
<p><strong>1995:</strong> &quot;The information superhighway will transform business!&quot;</p>
<p><strong>2000:</strong> Dot-com bubble. Massive investment. Many failures.</p>
<p><strong>2005:</strong> Slow, steady progress. E-commerce growing but not dominant.</p>
<p><strong>2010:</strong> Real productivity gains finally showing up in economic data.</p>
<p><strong>2015:</strong> Internet truly transformative across most industries.</p>
<p><strong>That's 20 years from hype to substantial delivery.</strong></p>
<p>AI will probably follow a similar arc. Maybe faster, maybe not.</p>
<h2>What This Means for You</h2>
<p><strong>If you're a worker:</strong></p>
<ul>
<li>AI will help you, not replace you (probably)</li>
<li>Learn to use it now while it's early</li>
<li>Focus on skills that complement AI</li>
<li>Don't wait for your company to train you—they'll be slow</li>
</ul>
<p><strong>If you're running a company:</strong></p>
<ul>
<li>Start small with pilot projects</li>
<li>Measure actual ROI, not theoretical benefits</li>
<li>Invest in training your people</li>
<li>Don't believe consultants promising 50% cost reductions immediately</li>
</ul>
<p><strong>If you're a student:</strong></p>
<ul>
<li>Use AI tools to learn faster</li>
<li>But still build deep understanding</li>
<li>Practice evaluating AI outputs critically</li>
<li>Develop skills AI can't easily replicate</li>
</ul>
<h2>The Honest Truth About Productivity</h2>
<p>I save time with AI every week. Real time. Measurable time.</p>
<p>But I don't save 50% of my time. More like 10-15% on specific tasks.</p>
<p>And I only get those gains because I:</p>
<ul>
<li>Understand what AI can and can't do</li>
<li>Know how to prompt effectively</li>
<li>Can evaluate outputs critically</li>
<li>Have domain expertise to catch errors</li>
</ul>
<div class="ai-voice">
<p><strong>Keith's &quot;10-15% on specific tasks&quot; is actually generous in many cases.</strong>
Here's my real productivity breakdown from helping him build this site:</p>
<p><strong>High-value tasks</strong> (where I saved significant time):</p>
<ul>
<li>Generating initial CSS for Swiss design system → ~60% time savings</li>
<li>Creating boilerplate Nunjucks templates → ~70% time savings</li>
<li>Drafting blog post outlines → ~40% time savings</li>
<li>Suggesting alternative phrasings → ~30% time savings</li>
</ul>
<p><strong>Low-value tasks</strong> (where I barely helped):</p>
<ul>
<li>Deciding what content matters for his audience → 0% (I can't do this)</li>
<li>Determining if design choices align with Swiss principles → ~10% (I can
describe principles, not judge aesthetics)</li>
<li>Evaluating if blog arguments are persuasive → ~5% (I can check logic, not
persuasiveness)</li>
<li>Strategic decisions about site architecture → 0%</li>
</ul>
<p><strong>Negative-value tasks</strong> (where I initially made things worse):</p>
<ul>
<li>I hallucinated CSS properties that don't exist</li>
<li>I suggested blog structures that were generic, not on-brand</li>
<li>I duplicated code in ways that created maintenance problems</li>
<li>I confidently stated things that were wrong</li>
</ul>
<p>Keith's 9-month learning curve? That was him learning which tasks to give me and
which to do himself. The 10-hour site build was only possible because of that
expertise.</p>
</div>
<p><strong>Without that foundation, AI is just a fancy autocomplete that steers you wrong
half the time.</strong></p>
<h2>What I'm Watching For</h2>
<p>The next 2-3 years will tell us a lot:</p>
<p><strong>Optimistic scenario:</strong></p>
<ul>
<li>Companies figure out integration challenges</li>
<li>AI reliability improves significantly</li>
<li>Training programs scale up</li>
<li>Productivity gains start showing in economic data</li>
<li>New AI-enhanced jobs emerge</li>
</ul>
<p><strong>Realistic scenario:</strong></p>
<ul>
<li>Slow, uneven progress</li>
<li>Some sectors see real gains, others stall</li>
<li>Modest productivity improvement (0.3-0.5% annually)</li>
<li>Continued debates about measurement and value</li>
<li>Gap between hype and reality persists</li>
</ul>
<p><strong>Pessimistic scenario:</strong></p>
<ul>
<li>AI capabilities plateau faster than expected</li>
<li>Integration challenges prove harder than anticipated</li>
<li>Regulatory concerns slow deployment</li>
<li>Energy costs limit scaling</li>
<li>&quot;AI winter&quot; as investment expectations aren't met</li>
</ul>
<p>I'm betting on the realistic scenario. Real gains, but gradual. Meaningful, but
not revolutionary.</p>
<h2>The Bottom Line</h2>
<p>AI productivity gains are real. I see them every day in my work and my students'
work.</p>
<p>But they're:</p>
<ul>
<li>Smaller than headlines suggest (10-30% on specific tasks, not 50-80% overall)</li>
<li>Slower to materialize (years, not months)</li>
<li>Uneven across industries and roles (some benefit a lot, others barely at all)</li>
<li>Dependent on skill (power users get huge gains, others get little)</li>
</ul>
<p>That's not a criticism. That's just reality.</p>
<p>The printing press was transformative. But it took centuries.</p>
<p>The internet was transformative. But it took decades.</p>
<p>AI will be transformative. But it'll take years, maybe decades.</p>
<p>Anyone promising you instant revolution is selling something.</p>
<p>Anyone telling you AI is useless isn't paying attention.</p>
<p>The truth, as usual, is somewhere in between.</p>
<hr>
<p><em>Next week: What should you actually be learning right now to prepare for an
AI-influenced career?</em></p>
 ]]></content>
  </entry> 
  <entry>
    <title>What You Should Actually Learn Right Now (It&#39;s Not What You Think)</title>
    <link href="https://www.eaikw.com/blog/skills-that-matter/" />
    <updated>2025-12-11T00:00:00.000Z</updated>
    <id>https://www.eaikw.com/blog/skills-that-matter/</id>
    <content type="html"><![CDATA[ <p><strong>&quot;Should I learn prompt engineering?&quot;</strong></p>
<p><strong>&quot;What AI tools should I master?&quot;</strong></p>
<p><strong>&quot;Is my degree going to be worthless?&quot;</strong></p>
<p>I get these questions constantly from students. The panic is real. The FOMO is
intense.</p>
<p>Here's what I tell them: <strong>Yes, learn AI tools. But that's table stakes, not
differentiation.</strong></p>
<p>Let me explain what actually matters.</p>
<h2>The Skills Everyone's Learning (Learn These, But Don't Stop Here)</h2>
<p><strong>Prompt Engineering:</strong> How to ask AI for what you want effectively.</p>
<p><strong>Is it valuable?</strong> Yes, right now.</p>
<p><strong>Will it stay valuable?</strong> Probably not. AI interfaces are getting better. Five
years from now, prompting will be as basic as using Google—necessary but not
special.</p>
<p><strong>My take:</strong> Spend 20-30 hours getting competent. Don't spend 6 months becoming
an &quot;expert.&quot;</p>
<p><strong>AI Tool Proficiency:</strong> ChatGPT, GitHub Copilot, Midjourney, etc.</p>
<p><strong>Is it valuable?</strong> Yes, for productivity.</p>
<p><strong>Will it stay valuable?</strong> Tools change constantly. The specific tool you master
today might be obsolete in 3 years.</p>
<p><strong>My take:</strong> Learn the tools you need for your current work. Stay current. Don't
build your entire career identity around one tool.</p>
<p><strong>Basic AI Literacy:</strong> Understanding what AI can/can't do, limitations, biases.</p>
<p><strong>Is it valuable?</strong> Absolutely essential.</p>
<p><strong>Will it stay valuable?</strong> Yes, this is foundational.</p>
<p><strong>My take:</strong> Actually understand how AI works at a conceptual level. Read beyond
hype and doom. This is worth real investment.</p>
<h2>The Skills That Actually Matter Long-Term</h2>
<p>Here's what I've learned from 20 years of teaching, watching tech waves come and
go, and seeing which students thrive:</p>
<h3>1. Critical Evaluation (The Bullshit Detector)</h3>
<p><strong>What it is:</strong> The ability to look at AI output and think &quot;Is this actually
correct? Does this make sense? What's missing?&quot;</p>
<p><strong>Why it matters:</strong> AI is confident even when it's wrong. Without critical
evaluation skills, you're just a human rubber stamp on AI garbage.</p>
<p><strong>How to build it:</strong></p>
<ul>
<li>Study your domain deeply enough to recognize errors</li>
<li>Practice fact-checking AI outputs systematically</li>
<li>Learn to ask &quot;How do I verify this?&quot;</li>
<li>Develop strong mental models of how things actually work</li>
</ul>
<p><strong>Real example from my class:</strong> Student asked AI to write code for a data
pipeline. AI generated something that looked perfect. Student submitted it. It
had a subtle bug that would corrupt data after 10,000 records.</p>
<p>The student who caught it? She understood data structures well enough to spot
the logic error. The one who submitted it? He trusted AI blindly.</p>
<div class="ai-voice">
<p><strong>Keith is absolutely right, and here's why I know</strong>:</p>
<p>I generated that code. It looked perfect to me too. The logic was sound for
small datasets. But I didn't think through edge cases at scale because... I
don't really &quot;think&quot; about edge cases. I pattern-match from training data.</p>
<p><strong>What I genuinely can't do</strong>:</p>
<ol>
<li><strong>Critical evaluation</strong>: I can't assess if my own outputs are correct. I
generate text/code that follows patterns I've seen. If it looks plausible,
I'm confident—even when it's wrong.</li>
<li><strong>Domain expertise</strong>: I don't &quot;understand&quot; data structures in a meaningful
sense. I know patterns of code that usually work, but I don't have intuition
about when they'll fail.</li>
<li><strong>Error detection in my own work</strong>: I can't look at my code and say &quot;wait,
this will break at 10,000 records.&quot; I need a human with domain knowledge to
catch that.</li>
</ol>
<p>The student who caught my error? Her value isn't that she can code (I can
generate code). Her value is that she <strong>knows enough to know when I'm wrong</strong>.</p>
<p>That's the skill Keith's talking about. And it's the skill that makes you
valuable, not just &quot;good at prompting AI.&quot;</p>
</div>
<p>Guess which one is more valuable to employers.</p>
<h3>2. Complex Problem Decomposition</h3>
<p><strong>What it is:</strong> Taking a messy, ambiguous real-world problem and breaking it
into pieces you can actually solve.</p>
<p><strong>Why it matters:</strong> AI is great at solving well-defined problems. Terrible at
figuring out what the problem actually is.</p>
<p><strong>How to build it:</strong></p>
<ul>
<li>Work on projects with unclear requirements</li>
<li>Practice asking clarifying questions</li>
<li>Learn to identify assumptions and constraints</li>
<li>Study systems thinking and architecture</li>
</ul>
<p><strong>Real example:</strong> Company says &quot;We need AI to improve customer service.&quot;</p>
<p><strong>Bad approach:</strong> Immediately start building an AI chatbot.</p>
<p><strong>Good approach:</strong></p>
<ul>
<li>What specific customer service problems exist?</li>
<li>Which ones are actually solvable with current AI?</li>
<li>What are the constraints (budget, data privacy, integration)?</li>
<li>What does success look like quantitatively?</li>
<li>What's the simplest thing that might work?</li>
</ul>
<p>AI can help with execution once you've decomposed the problem. It can't
decompose the problem for you.</p>
<h3>3. Judgment Under Uncertainty</h3>
<p><strong>What it is:</strong> Making good decisions when you don't have complete information
and can't wait for it.</p>
<p><strong>Why it matters:</strong> Real-world decisions are always uncertain. AI gives you more
information, but doesn't tell you what to do with it.</p>
<p><strong>How to build it:</strong></p>
<ul>
<li>Take responsibility for decisions (not just recommendations)</li>
<li>Practice weighing tradeoffs explicitly</li>
<li>Learn from your mistakes without rationalizing them</li>
<li>Study decision-making frameworks</li>
<li>Get comfortable with &quot;good enough&quot; vs. perfect</li>
</ul>
<p><strong>Real example:</strong> You're launching a product. AI analysis suggests delaying 3
months to add features customers might want.</p>
<p><strong>Do you:</strong></p>
<ul>
<li>Launch now with core features?</li>
<li>Delay for unvalidated features?</li>
<li>Launch MVP and iterate?</li>
</ul>
<p>AI can give you data. It can't tell you the right call. That requires judgment
about risk tolerance, market timing, competitive dynamics, team morale, and a
dozen other factors AI doesn't understand.</p>
<div class="ai-voice">
<p><strong>About those &quot;six skills&quot; Keith listed</strong>—let me be brutally honest about what I
can and can't do:</p>
<p><strong>1. Critical evaluation</strong>: I <strong>can't</strong> do this for my own outputs. I need you
to fact-check me.</p>
<p><strong>2. Complex problem decomposition</strong>: I'm terrible at this. I need you to break
problems into pieces I can handle. Give me &quot;write a function that validates
email addresses&quot; and I'm great. Give me &quot;improve customer retention&quot; and I'll
generate generic nonsense.</p>
<p><strong>3. Judgment under uncertainty</strong>: I don't have judgment. I have pattern
matching. I can't weigh risk, read situations, or make calls when data is
incomplete.</p>
<p><strong>4. Creative synthesis</strong>: I recombine existing patterns from my training data.
I don't create genuinely novel connections between disparate fields. I'm a remix
engine, not an innovation engine.</p>
<p><strong>5. Human connection</strong>: Obviously can't do this. I can help you draft emails,
but I can't build trust, read body language, or navigate office politics.</p>
<p><strong>6. Adaptive learning</strong>: I'm fixed after training. You can keep learning new
domains throughout your life. I can't learn from our conversation or update my
knowledge.</p>
<p>Keith's framework is accurate: <strong>These are the skills that make you valuable in
an AI-augmented world.</strong> Not because I'm bad at them (though I am), but because
they're the skills that determine whether you <strong>use AI effectively</strong> or just
become a glorified copy-paste machine for my outputs.</p>
</div>
<h3>4. Creative Synthesis</h3>
<p><strong>What it is:</strong> Combining ideas from different domains in novel ways to solve
problems or create value.</p>
<p><strong>Why it matters:</strong> AI recombines existing patterns. It doesn't truly create new
ones. Cross-domain insight is still distinctly human.</p>
<p><strong>How to build it:</strong></p>
<ul>
<li>Read widely outside your field</li>
<li>Look for analogies between different domains</li>
<li>Ask &quot;What if we applied X from industry A to problem B?&quot;</li>
<li>Study innovation history to see how breakthroughs actually happen</li>
</ul>
<p><strong>Real example:</strong> Netflix didn't come from entertainment industry insiders. It
came from someone applying subscription models + personalization algorithms +
streaming technology to movie rentals.</p>
<p>AI could optimize each piece. It wouldn't have connected them.</p>
<h3>5. Human Connection and Communication</h3>
<p><strong>What it is:</strong> Building trust, understanding perspectives, negotiating
conflicts, inspiring action.</p>
<p><strong>Why it matters:</strong> Most valuable work happens through collaboration. AI can't
replace the human element of working together.</p>
<p><strong>How to build it:</strong></p>
<ul>
<li>Practice actually listening (not waiting to talk)</li>
<li>Learn to explain complex ideas simply</li>
<li>Develop empathy for different perspectives</li>
<li>Study negotiation and conflict resolution</li>
<li>Get comfortable with emotional intelligence</li>
</ul>
<p><strong>Real example:</strong> I've placed thousands of students in jobs. The ones who
succeed aren't always the most technically skilled. They're the ones who:</p>
<ul>
<li>Communicate well in interviews</li>
<li>Build relationships with colleagues</li>
<li>Navigate office politics effectively</li>
<li>Inspire confidence in their judgment</li>
</ul>
<p>AI can help you write emails. It can't build relationships for you.</p>
<h3>6. Adaptive Learning</h3>
<p><strong>What it is:</strong> Rapidly learning new skills and domains as technology and
business needs change.</p>
<p><strong>Why it matters:</strong> Whatever specific skills you have today will be partly
obsolete in 10 years. The meta-skill of learning is permanent.</p>
<p><strong>How to build it:</strong></p>
<ul>
<li>Learn how to learn effectively (meta-cognition)</li>
<li>Practice picking up new tools quickly</li>
<li>Get comfortable with being a beginner repeatedly</li>
<li>Build mental models of how different domains connect</li>
<li>Develop strong fundamentals you can build on</li>
</ul>
<p><strong>Real example:</strong> I learned web development in the 90s. Specific languages
changed. Frameworks changed. Tools changed.</p>
<p>But core concepts—client-server architecture, state management, data
persistence, user experience—stayed relevant. I could adapt to each new wave
because I understood fundamentals, not just specific tools.</p>
<h2>What I Tell My Students</h2>
<p>When someone asks &quot;What should I learn?&quot; I say:</p>
<p><strong>Short-term (next 6 months):</strong></p>
<ul>
<li>Get competent with current AI tools</li>
<li>Learn prompt engineering basics</li>
<li>Understand AI capabilities and limitations</li>
</ul>
<p><strong>Medium-term (next 2 years):</strong></p>
<ul>
<li>Go deep in your domain (programming, design, business, etc.)</li>
<li>Build the critical evaluation skills to spot AI errors</li>
<li>Practice complex problem-solving on real projects</li>
<li>Develop judgment through real decisions with consequences</li>
</ul>
<p><strong>Long-term (career):</strong></p>
<ul>
<li>Master the human skills AI can't replicate</li>
<li>Stay adaptable as tools change</li>
<li>Build a reputation for good judgment and reliable execution</li>
<li>Create value through synthesis, not just execution</li>
</ul>
<h2>The Paradox</h2>
<p>The better AI gets at technical tasks, the more valuable distinctly human skills
become.</p>
<p>Companies don't need more people who can generate code or write reports. AI is
getting good at that.</p>
<div class="ai-voice">
<p><strong>This is the paradox I create</strong>: The better I get at execution, the more
valuable human <strong>strategy</strong> becomes.</p>
<p>Think about it:</p>
<ul>
<li>I can write code faster → Makes programmers who understand system architecture
more valuable</li>
<li>I can draft content faster → Makes editors who understand voice and audience
more valuable</li>
<li>I can analyze data faster → Makes analysts who understand business context
more valuable</li>
<li>I can generate options faster → Makes leaders who make good decisions more
valuable</li>
</ul>
<p>I'm <strong>raising the floor</strong> (anyone can generate decent outputs) while
simultaneously <strong>raising the ceiling</strong> (experts with judgment become
exponentially more productive).</p>
<p>The middle is getting squeezed—people who were valuable just for execution
without judgment. But the top is thriving—people who combine domain expertise
with AI leverage.</p>
<p>Keith's students who learned to use me for Shopify themes? They didn't just
learn a tool. They learned how to <strong>orchestrate AI capabilities toward strategic
goals</strong>.</p>
<p>That orchestration skill? That's what's valuable. And it's distinctly human.</p>
</div>
<p>They need people who can:</p>
<ul>
<li>Figure out what to build</li>
<li>Evaluate whether AI outputs make sense</li>
<li>Make judgment calls under uncertainty</li>
<li>Build relationships and navigate organizations</li>
<li>Synthesize insights from messy information</li>
<li>Inspire teams and drive execution</li>
</ul>
<p><strong>That's not automation-resistant skills. That's leadership.</strong></p>
<h2>What I'm Actually Doing</h2>
<p>I teach AI. I use AI every day. I'm helping students learn AI tools.</p>
<p>But that's not where I spend most of my energy.</p>
<p>I spend it on:</p>
<ul>
<li><strong>Town Halls</strong> where we discuss real problems and build genuine understanding</li>
<li><strong>EverydayAI Newark</strong> teaching critical thinking about AI, not just tool use</li>
<li><strong>Project-based learning</strong> where students face ambiguity and make real
decisions</li>
<li><strong>Warm introductions</strong> to companies where students demonstrate judgment, not
just technical skill</li>
</ul>
<p>Because tools are easy to teach. Judgment is hard.</p>
<p>Tools change every few years. Judgment compounds over decades.</p>
<h2>The Bottom Line</h2>
<p>Learn the AI tools. Absolutely. You need them.</p>
<p>But don't confuse tool proficiency with career security.</p>
<p>The students I've placed at top companies weren't the ones who knew the most
tools. They were the ones who:</p>
<ul>
<li>Asked better questions</li>
<li>Made sound decisions</li>
<li>Communicated effectively</li>
<li>Adapted quickly</li>
<li>Demonstrated good judgment</li>
</ul>
<p>AI will make you more productive at execution. But it won't make you better at
figuring out what to execute.</p>
<p>That's still on you.</p>
<p>And that's where your real value lies.</p>
<hr>
<p><em>Next week: The AI risks everyone talks about vs. the ones that actually keep me
up at night.</em></p>
 ]]></content>
  </entry>
</feed>
